{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e8a482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9f9a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import os\\nimport sys\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow.compat.v1 as tf\\n\\ntf.disable_v2_behavior()\";\n",
       "                var nbb_formatted_code = \"import os\\nimport sys\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow.compat.v1 as tf\\n\\ntf.disable_v2_behavior()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccbea12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"class MLP:\\n    def __init__(self, vocab_size, hidden_size):\\n        self._vocab_size = vocab_size\\n        self._hidden_size = hidden_size\\n\\n    def build_graph(self):\\n        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\\n        self._real_Y = tf.placeholder(tf.int32, shape=[None, ])\\n        weights_1 = tf.get_variable(\\n            name=\\\"weight_input_hidden\\\",\\n            shape=(self._vocab_size, self._hidden_size),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n        biases_1 = tf.get_variable(\\n            name=\\\"biases_input_hidden\\\",\\n            shape=(self._hidden_size),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        weights_2 = tf.get_variable(\\n            name=\\\"weights_hidden_output\\\",\\n            shape=(self._hidden_size, NUM_CLASSES),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        biases_2 = tf.get_variable(\\n            name=\\\"biases_hidden_output\\\",\\n            shape=(NUM_CLASSES),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        hidden = tf.matmul(self._X, weights_1) + biases_1  # Net input\\n        hidden = tf.sigmoid(hidden)  # activation function.\\n\\n        logits = tf.matmul(hidden, weights_2) + biases_2\\n\\n        labels_one_hot = tf.one_hot(\\n            indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32\\n        )\\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\\n            labels=labels_one_hot, logits=logits\\n        )\\n        loss = tf.reduce_mean(loss)\\n        probs = tf.nn.softmax(logits)  # difference between 2 distributions.\\n        predicted_labels = tf.argmax(probs, axis=1)\\n        predicted_labels = tf.squeeze(predicted_labels)\\n        return predicted_labels, loss\\n\\n    def trainer(self, loss, learning_rate):\\n        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\\n        return train_op\";\n",
       "                var nbb_formatted_code = \"class MLP:\\n    def __init__(self, vocab_size, hidden_size):\\n        self._vocab_size = vocab_size\\n        self._hidden_size = hidden_size\\n\\n    def build_graph(self):\\n        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\\n        self._real_Y = tf.placeholder(\\n            tf.int32,\\n            shape=[\\n                None,\\n            ],\\n        )\\n        weights_1 = tf.get_variable(\\n            name=\\\"weight_input_hidden\\\",\\n            shape=(self._vocab_size, self._hidden_size),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n        biases_1 = tf.get_variable(\\n            name=\\\"biases_input_hidden\\\",\\n            shape=(self._hidden_size),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        weights_2 = tf.get_variable(\\n            name=\\\"weights_hidden_output\\\",\\n            shape=(self._hidden_size, NUM_CLASSES),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        biases_2 = tf.get_variable(\\n            name=\\\"biases_hidden_output\\\",\\n            shape=(NUM_CLASSES),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        hidden = tf.matmul(self._X, weights_1) + biases_1  # Net input\\n        hidden = tf.sigmoid(hidden)  # activation function.\\n\\n        logits = tf.matmul(hidden, weights_2) + biases_2\\n\\n        labels_one_hot = tf.one_hot(\\n            indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32\\n        )\\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\\n            labels=labels_one_hot, logits=logits\\n        )\\n        loss = tf.reduce_mean(loss)\\n        probs = tf.nn.softmax(logits)  # difference between 2 distributions.\\n        predicted_labels = tf.argmax(probs, axis=1)\\n        predicted_labels = tf.squeeze(predicted_labels)\\n        return predicted_labels, loss\\n\\n    def trainer(self, loss, learning_rate):\\n        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\\n        return train_op\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "    def build_graph(self):\n",
    "        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
    "        self._real_Y = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[\n",
    "                None,\n",
    "            ],\n",
    "        )\n",
    "        weights_1 = tf.get_variable(\n",
    "            name=\"weight_input_hidden\",\n",
    "            shape=(self._vocab_size, self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "        biases_1 = tf.get_variable(\n",
    "            name=\"biases_input_hidden\",\n",
    "            shape=(self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "\n",
    "        weights_2 = tf.get_variable(\n",
    "            name=\"weights_hidden_output\",\n",
    "            shape=(self._hidden_size, NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "\n",
    "        biases_2 = tf.get_variable(\n",
    "            name=\"biases_hidden_output\",\n",
    "            shape=(NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "\n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1  # Net input\n",
    "        hidden = tf.sigmoid(hidden)  # activation function.\n",
    "\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "\n",
    "        labels_one_hot = tf.one_hot(\n",
    "            indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32\n",
    "        )\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=labels_one_hot, logits=logits\n",
    "        )\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        probs = tf.nn.softmax(logits)  # difference between 2 distributions.\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        return predicted_labels, loss\n",
    "\n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005069b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class DataReader:\\n    def __init__(self, data_path, batch_size, vocab_size):\\n        self._batch_size = batch_size\\n        with open(data_path) as f:\\n            d_lines = f.read().splitlines()\\n\\n        self._data = []\\n        self._labels = []\\n        for data_id, line in enumerate(d_lines):\\n            vector = [0.0 for _ in range(vocab_size)]\\n            features = line.split(\\\"<fff>\\\")\\n            label, doc_id = int(features[0]), int(features[1])\\n            tokens = features[2].split()\\n            for token in tokens:\\n                index, value = int(token.split(\\\":\\\")[0]), float(token.split(\\\":\\\")[1])\\n                vector[index] = value\\n            self._data.append(vector)\\n            self._labels.append(label)\\n\\n        self._data = np.array(self._data)\\n        self._labels = np.array(self._labels)\\n        self._num_epoch = 0\\n        self._batch_id = 0\\n\\n    def next_batch(self):\\n        start = self._batch_id * self._batch_size\\n        end = start + self._batch_size\\n        self._batch_id += 1\\n\\n        if end + self._batch_size > len(self._data):\\n            end = len(self._data)\\n            self._num_epoch += 1\\n            self._batch_id = 0\\n            indices = list(range(len(self._data)))\\n            random.seed(2021)\\n            random.shuffle(indices)\\n            self._data, self._labels = self._data[indices], self._labels[indices]\\n        return self._data[start:end], self._labels[start:end]\";\n",
       "                var nbb_formatted_code = \"class DataReader:\\n    def __init__(self, data_path, batch_size, vocab_size):\\n        self._batch_size = batch_size\\n        with open(data_path) as f:\\n            d_lines = f.read().splitlines()\\n\\n        self._data = []\\n        self._labels = []\\n        for data_id, line in enumerate(d_lines):\\n            vector = [0.0 for _ in range(vocab_size)]\\n            features = line.split(\\\"<fff>\\\")\\n            label, doc_id = int(features[0]), int(features[1])\\n            tokens = features[2].split()\\n            for token in tokens:\\n                index, value = int(token.split(\\\":\\\")[0]), float(token.split(\\\":\\\")[1])\\n                vector[index] = value\\n            self._data.append(vector)\\n            self._labels.append(label)\\n\\n        self._data = np.array(self._data)\\n        self._labels = np.array(self._labels)\\n        self._num_epoch = 0\\n        self._batch_id = 0\\n\\n    def next_batch(self):\\n        start = self._batch_id * self._batch_size\\n        end = start + self._batch_size\\n        self._batch_id += 1\\n\\n        if end + self._batch_size > len(self._data):\\n            end = len(self._data)\\n            self._num_epoch += 1\\n            self._batch_id = 0\\n            indices = list(range(len(self._data)))\\n            random.seed(2021)\\n            random.shuffle(indices)\\n            self._data, self._labels = self._data[indices], self._labels[indices]\\n        return self._data[start:end], self._labels[start:end]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "\n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for data_id, line in enumerate(d_lines):\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split(\"<fff>\")\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            for token in tokens:\n",
    "                index, value = int(token.split(\":\")[0]), float(token.split(\":\")[1])\n",
    "                vector[index] = value\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "\n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = start + self._batch_size\n",
    "        self._batch_id += 1\n",
    "\n",
    "        if end + self._batch_size > len(self._data):\n",
    "            end = len(self._data)\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2021)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "        return self._data[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de0ddc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def load_dataset(vocab_size):\\n    train_data_reader = DataReader(\\n        data_path=os.getcwd() + \\\"/20news-bydate/20news-train-tf-idf.txt\\\",\\n        batch_size=50,\\n        vocab_size=vocab_size,\\n    )\\n\\n    test_data_reader = DataReader(\\n        data_path=os.getcwd() + \\\"/20news-bydate/20news-test-tf-idf.txt\\\",\\n        batch_size=50,\\n        vocab_size=vocab_size,\\n    )\\n\\n    return train_data_reader, test_data_reader\";\n",
       "                var nbb_formatted_code = \"def load_dataset(vocab_size):\\n    train_data_reader = DataReader(\\n        data_path=os.getcwd() + \\\"/20news-bydate/20news-train-tf-idf.txt\\\",\\n        batch_size=50,\\n        vocab_size=vocab_size,\\n    )\\n\\n    test_data_reader = DataReader(\\n        data_path=os.getcwd() + \\\"/20news-bydate/20news-test-tf-idf.txt\\\",\\n        batch_size=50,\\n        vocab_size=vocab_size,\\n    )\\n\\n    return train_data_reader, test_data_reader\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_dataset(vocab_size):\n",
    "    train_data_reader = DataReader(\n",
    "        data_path=os.getcwd() + \"/20news-bydate/20news-train-tf-idf.txt\",\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size,\n",
    "    )\n",
    "\n",
    "    test_data_reader = DataReader(\n",
    "        data_path=os.getcwd() + \"/20news-bydate/20news-test-tf-idf.txt\",\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size,\n",
    "    )\n",
    "\n",
    "    return train_data_reader, test_data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5854d24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"def save_parameters(name, value, epoch):\\n    filename = name.replace(\\\":\\\", \\\"-colon-\\\") + \\\"-epoch-{}.txt\\\".format(epoch)\\n    if len(value.shape) == 1:\\n        string_form = \\\",\\\".join([str(number) for number in value])\\n    else:\\n        string_form = \\\"\\\\n\\\".join(\\n            [\\n                \\\",\\\".join([str(number) for number in value[row]])\\n                for row in range(value.shape[0])\\n            ]\\n        )\\n    with open(os.getcwd() + \\\"/saved-paras/\\\" + filename, \\\"w\\\") as f:\\n        f.write(string_form)\";\n",
       "                var nbb_formatted_code = \"def save_parameters(name, value, epoch):\\n    filename = name.replace(\\\":\\\", \\\"-colon-\\\") + \\\"-epoch-{}.txt\\\".format(epoch)\\n    if len(value.shape) == 1:\\n        string_form = \\\",\\\".join([str(number) for number in value])\\n    else:\\n        string_form = \\\"\\\\n\\\".join(\\n            [\\n                \\\",\\\".join([str(number) for number in value[row]])\\n                for row in range(value.shape[0])\\n            ]\\n        )\\n    with open(os.getcwd() + \\\"/saved-paras/\\\" + filename, \\\"w\\\") as f:\\n        f.write(string_form)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_parameters(name, value, epoch):\n",
    "    filename = name.replace(\":\", \"-colon-\") + \"-epoch-{}.txt\".format(epoch)\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = \",\".join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = \"\\n\".join(\n",
    "            [\n",
    "                \",\".join([str(number) for number in value[row]])\n",
    "                for row in range(value.shape[0])\n",
    "            ]\n",
    "        )\n",
    "    with open(os.getcwd() + \"/saved-paras/\" + filename, \"w\") as f:\n",
    "        f.write(string_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231c1bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def restore_parameters(name, epoch):\\n    filename = name.replace(\\\":\\\", \\\"-colon-\\\") + \\\"-epoch-{}.txt\\\".format(epoch)\\n    with open(os.getcwd() + \\\"/saved-paras/\\\" + filename, \\\"r\\\") as f:\\n        lines = f.read().splitlines()\\n    if len(lines) == 1:\\n        value = [float(number) for number in lines[0].split(\\\",\\\")]\\n    else:\\n        value = [\\n            [float(number) for number in lines[row].split(\\\",\\\")]\\n            for row in range(len(lines))\\n        ]\\n    return value\";\n",
       "                var nbb_formatted_code = \"def restore_parameters(name, epoch):\\n    filename = name.replace(\\\":\\\", \\\"-colon-\\\") + \\\"-epoch-{}.txt\\\".format(epoch)\\n    with open(os.getcwd() + \\\"/saved-paras/\\\" + filename, \\\"r\\\") as f:\\n        lines = f.read().splitlines()\\n    if len(lines) == 1:\\n        value = [float(number) for number in lines[0].split(\\\",\\\")]\\n    else:\\n        value = [\\n            [float(number) for number in lines[row].split(\\\",\\\")]\\n            for row in range(len(lines))\\n        ]\\n    return value\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def restore_parameters(name, epoch):\n",
    "    filename = name.replace(\":\", \"-colon-\") + \"-epoch-{}.txt\".format(epoch)\n",
    "    with open(os.getcwd() + \"/saved-paras/\" + filename, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(\",\")]\n",
    "    else:\n",
    "        value = [\n",
    "            [float(number) for number in lines[row].split(\",\")]\n",
    "            for row in range(len(lines))\n",
    "        ]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "967f94ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 12:51:35.751559: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 11.104410171508789\n",
      "step: 1, loss: 0.9813809990882874\n",
      "step: 2, loss: 0.0006089996313676238\n",
      "step: 3, loss: 2.036018940998474e-06\n",
      "step: 4, loss: 1.907348234908568e-08\n",
      "step: 5, loss: 0.0\n",
      "step: 6, loss: 0.0\n",
      "step: 7, loss: 0.0\n",
      "step: 8, loss: 0.0\n",
      "step: 9, loss: 14.782851219177246\n",
      "step: 10, loss: 35.11439514160156\n",
      "step: 11, loss: 31.772096633911133\n",
      "step: 12, loss: 26.245704650878906\n",
      "step: 13, loss: 20.384763717651367\n",
      "step: 14, loss: 12.772749900817871\n",
      "step: 15, loss: 6.130094528198242\n",
      "step: 16, loss: 0.990744948387146\n",
      "step: 17, loss: 0.06624089926481247\n",
      "step: 18, loss: 0.0001084230825654231\n",
      "step: 19, loss: 1.053785354088177e-06\n",
      "step: 20, loss: 0.0\n",
      "step: 21, loss: 24.634183883666992\n",
      "step: 22, loss: 32.63716506958008\n",
      "step: 23, loss: 29.58038902282715\n",
      "step: 24, loss: 26.681072235107422\n",
      "step: 25, loss: 22.07748794555664\n",
      "step: 26, loss: 17.576826095581055\n",
      "step: 27, loss: 12.419458389282227\n",
      "step: 28, loss: 8.495309829711914\n",
      "step: 29, loss: 4.281155109405518\n",
      "step: 30, loss: 1.077122688293457\n",
      "step: 31, loss: 0.046487484127283096\n",
      "step: 32, loss: 0.06662814319133759\n",
      "step: 33, loss: 17.661041259765625\n",
      "step: 34, loss: 19.809751510620117\n",
      "step: 35, loss: 18.38697052001953\n",
      "step: 36, loss: 16.343286514282227\n",
      "step: 37, loss: 14.193692207336426\n",
      "step: 38, loss: 11.481032371520996\n",
      "step: 39, loss: 8.820477485656738\n",
      "step: 40, loss: 6.483560562133789\n",
      "step: 41, loss: 4.291399002075195\n",
      "step: 42, loss: 2.39984130859375\n",
      "step: 43, loss: 0.8817689418792725\n",
      "step: 44, loss: 1.7890151739120483\n",
      "step: 45, loss: 15.761369705200195\n",
      "step: 46, loss: 14.412138938903809\n",
      "step: 47, loss: 14.547990798950195\n",
      "step: 48, loss: 12.690193176269531\n",
      "step: 49, loss: 11.249582290649414\n",
      "step: 50, loss: 9.529751777648926\n",
      "step: 51, loss: 7.860347747802734\n",
      "step: 52, loss: 6.282916069030762\n",
      "step: 53, loss: 4.623212814331055\n",
      "step: 54, loss: 3.624711275100708\n",
      "step: 55, loss: 2.379946231842041\n",
      "step: 56, loss: 5.7982707023620605\n",
      "step: 57, loss: 7.969674110412598\n",
      "step: 58, loss: 7.420160293579102\n",
      "step: 59, loss: 6.6519951820373535\n",
      "step: 60, loss: 5.669539928436279\n",
      "step: 61, loss: 4.632778167724609\n",
      "step: 62, loss: 3.725177526473999\n",
      "step: 63, loss: 2.813662528991699\n",
      "step: 64, loss: 2.026445150375366\n",
      "step: 65, loss: 1.4851789474487305\n",
      "step: 66, loss: 0.9275364875793457\n",
      "step: 67, loss: 0.5142601132392883\n",
      "step: 68, loss: 6.940255641937256\n",
      "step: 69, loss: 10.02556324005127\n",
      "step: 70, loss: 9.5852689743042\n",
      "step: 71, loss: 9.182806015014648\n",
      "step: 72, loss: 8.593782424926758\n",
      "step: 73, loss: 7.833108901977539\n",
      "step: 74, loss: 7.125202178955078\n",
      "step: 75, loss: 6.336867809295654\n",
      "step: 76, loss: 5.343020439147949\n",
      "step: 77, loss: 4.796245098114014\n",
      "step: 78, loss: 3.8779659271240234\n",
      "step: 79, loss: 3.3723254203796387\n",
      "step: 80, loss: 8.288185119628906\n",
      "step: 81, loss: 7.360990047454834\n",
      "step: 82, loss: 6.262777328491211\n",
      "step: 83, loss: 5.734777927398682\n",
      "step: 84, loss: 5.106266498565674\n",
      "step: 85, loss: 4.451646327972412\n",
      "step: 86, loss: 4.3172101974487305\n",
      "step: 87, loss: 3.7200253009796143\n",
      "step: 88, loss: 3.5441501140594482\n",
      "step: 89, loss: 3.1838440895080566\n",
      "step: 90, loss: 2.9038591384887695\n",
      "step: 91, loss: 2.855569362640381\n",
      "step: 92, loss: 4.581860065460205\n",
      "step: 93, loss: 4.362789154052734\n",
      "step: 94, loss: 4.2939324378967285\n",
      "step: 95, loss: 4.009975910186768\n",
      "step: 96, loss: 3.8524231910705566\n",
      "step: 97, loss: 3.610715389251709\n",
      "step: 98, loss: 3.467951774597168\n",
      "step: 99, loss: 3.2447023391723633\n",
      "step: 100, loss: 3.0144641399383545\n",
      "step: 101, loss: 2.8042643070220947\n",
      "step: 102, loss: 2.602804660797119\n",
      "step: 103, loss: 3.4255785942077637\n",
      "step: 104, loss: 9.667693138122559\n",
      "step: 105, loss: 9.371589660644531\n",
      "step: 106, loss: 9.159610748291016\n",
      "step: 107, loss: 8.986766815185547\n",
      "step: 108, loss: 8.761035919189453\n",
      "step: 109, loss: 8.62346363067627\n",
      "step: 110, loss: 8.202046394348145\n",
      "step: 111, loss: 8.029415130615234\n",
      "step: 112, loss: 7.658167839050293\n",
      "step: 113, loss: 7.415939807891846\n",
      "step: 114, loss: 7.117061614990234\n",
      "step: 115, loss: 7.296212196350098\n",
      "step: 116, loss: 9.099483489990234\n",
      "step: 117, loss: 8.868365287780762\n",
      "step: 118, loss: 8.647225379943848\n",
      "step: 119, loss: 8.425662994384766\n",
      "step: 120, loss: 8.142382621765137\n",
      "step: 121, loss: 7.921273231506348\n",
      "step: 122, loss: 7.577844142913818\n",
      "step: 123, loss: 7.280574321746826\n",
      "step: 124, loss: 6.969094753265381\n",
      "step: 125, loss: 6.360687255859375\n",
      "step: 126, loss: 5.934698104858398\n",
      "step: 127, loss: 5.75192928314209\n",
      "step: 128, loss: 7.449451446533203\n",
      "step: 129, loss: 7.2609405517578125\n",
      "step: 130, loss: 7.060606479644775\n",
      "step: 131, loss: 6.803619861602783\n",
      "step: 132, loss: 6.532403469085693\n",
      "step: 133, loss: 6.227334499359131\n",
      "step: 134, loss: 5.907822132110596\n",
      "step: 135, loss: 5.554500579833984\n",
      "step: 136, loss: 5.135530948638916\n",
      "step: 137, loss: 4.71271276473999\n",
      "step: 138, loss: 4.394721508026123\n",
      "step: 139, loss: 5.84991455078125\n",
      "step: 140, loss: 10.068353652954102\n",
      "step: 141, loss: 9.950401306152344\n",
      "step: 142, loss: 9.762154579162598\n",
      "step: 143, loss: 9.299692153930664\n",
      "step: 144, loss: 9.096571922302246\n",
      "step: 145, loss: 8.704146385192871\n",
      "step: 146, loss: 8.385232925415039\n",
      "step: 147, loss: 8.078739166259766\n",
      "step: 148, loss: 7.7774248123168945\n",
      "step: 149, loss: 7.450246810913086\n",
      "step: 150, loss: 7.294276714324951\n",
      "step: 151, loss: 9.08122444152832\n",
      "step: 152, loss: 10.480145454406738\n",
      "step: 153, loss: 9.826123237609863\n",
      "step: 154, loss: 9.500129699707031\n",
      "step: 155, loss: 9.025385856628418\n",
      "step: 156, loss: 8.767553329467773\n",
      "step: 157, loss: 8.581266403198242\n",
      "step: 158, loss: 8.243195533752441\n",
      "step: 159, loss: 7.914384841918945\n",
      "step: 160, loss: 7.59139347076416\n",
      "step: 161, loss: 7.296038627624512\n",
      "step: 162, loss: 6.973011016845703\n",
      "step: 163, loss: 8.467680931091309\n",
      "step: 164, loss: 9.585335731506348\n",
      "step: 165, loss: 9.351245880126953\n",
      "step: 166, loss: 9.118038177490234\n",
      "step: 167, loss: 8.911609649658203\n",
      "step: 168, loss: 8.597238540649414\n",
      "step: 169, loss: 8.280806541442871\n",
      "step: 170, loss: 7.961629867553711\n",
      "step: 171, loss: 7.559450149536133\n",
      "step: 172, loss: 7.258590221405029\n",
      "step: 173, loss: 6.8595051765441895\n",
      "step: 174, loss: 6.126398086547852\n",
      "step: 175, loss: 5.511171817779541\n",
      "step: 176, loss: 4.8929972648620605\n",
      "step: 177, loss: 4.517349720001221\n",
      "step: 178, loss: 4.033371448516846\n",
      "step: 179, loss: 3.432831048965454\n",
      "step: 180, loss: 2.8849759101867676\n",
      "step: 181, loss: 2.271756887435913\n",
      "step: 182, loss: 1.6881585121154785\n",
      "step: 183, loss: 1.155137538909912\n",
      "step: 184, loss: 0.7160578370094299\n",
      "step: 185, loss: 0.4286899268627167\n",
      "step: 186, loss: 0.2675268352031708\n",
      "step: 187, loss: 6.751935958862305\n",
      "step: 188, loss: 8.88895320892334\n",
      "step: 189, loss: 8.701713562011719\n",
      "step: 190, loss: 8.54298210144043\n",
      "step: 191, loss: 7.8315043449401855\n",
      "step: 192, loss: 7.249422073364258\n",
      "step: 193, loss: 6.588357448577881\n",
      "step: 194, loss: 5.8401594161987305\n",
      "step: 195, loss: 5.011842250823975\n",
      "step: 196, loss: 4.175630569458008\n",
      "step: 197, loss: 3.2965967655181885\n",
      "step: 198, loss: 4.722146511077881\n",
      "step: 199, loss: 4.688808441162109\n",
      "step: 200, loss: 4.201821804046631\n",
      "step: 201, loss: 3.7217013835906982\n",
      "step: 202, loss: 3.288378953933716\n",
      "step: 203, loss: 2.918851852416992\n",
      "step: 204, loss: 2.4354732036590576\n",
      "step: 205, loss: 1.9782891273498535\n",
      "step: 206, loss: 1.4801397323608398\n",
      "step: 207, loss: 1.0474056005477905\n",
      "step: 208, loss: 0.6867325305938721\n",
      "step: 209, loss: 7.435183048248291\n",
      "step: 210, loss: 12.858869552612305\n",
      "step: 211, loss: 12.701613426208496\n",
      "step: 212, loss: 12.478494644165039\n",
      "step: 213, loss: 12.038585662841797\n",
      "step: 214, loss: 11.336546897888184\n",
      "step: 215, loss: 10.09965705871582\n",
      "step: 216, loss: 9.347718238830566\n",
      "step: 217, loss: 8.0381498336792\n",
      "step: 218, loss: 9.52623176574707\n",
      "step: 219, loss: 15.281423568725586\n",
      "step: 220, loss: 13.099096298217773\n",
      "step: 221, loss: 12.19359016418457\n",
      "step: 222, loss: 10.697700500488281\n",
      "step: 223, loss: 10.141587257385254\n",
      "step: 224, loss: 9.650694847106934\n",
      "step: 225, loss: 3.3029651641845703\n",
      "step: 226, loss: 3.117654800415039\n",
      "step: 227, loss: 2.7510733604431152\n",
      "step: 228, loss: 2.9141485691070557\n",
      "step: 229, loss: 2.8917086124420166\n",
      "step: 230, loss: 2.966104745864868\n",
      "step: 231, loss: 2.7280848026275635\n",
      "step: 232, loss: 2.815472364425659\n",
      "step: 233, loss: 3.5936083793640137\n",
      "step: 234, loss: 2.630190372467041\n",
      "step: 235, loss: 2.995356559753418\n",
      "step: 236, loss: 3.0611331462860107\n",
      "step: 237, loss: 3.1148929595947266\n",
      "step: 238, loss: 2.9215734004974365\n",
      "step: 239, loss: 2.6444661617279053\n",
      "step: 240, loss: 2.4757919311523438\n",
      "step: 241, loss: 2.644240617752075\n",
      "step: 242, loss: 2.710162878036499\n",
      "step: 243, loss: 2.5573787689208984\n",
      "step: 244, loss: 2.511535167694092\n",
      "step: 245, loss: 2.3153228759765625\n",
      "step: 246, loss: 2.4598960876464844\n",
      "step: 247, loss: 2.1861257553100586\n",
      "step: 248, loss: 2.8568079471588135\n",
      "step: 249, loss: 2.4554152488708496\n",
      "step: 250, loss: 2.164566993713379\n",
      "step: 251, loss: 2.2517170906066895\n",
      "step: 252, loss: 2.03414249420166\n",
      "step: 253, loss: 2.3485589027404785\n",
      "step: 254, loss: 2.0011422634124756\n",
      "step: 255, loss: 1.8149224519729614\n",
      "step: 256, loss: 2.272900342941284\n",
      "step: 257, loss: 2.4679861068725586\n",
      "step: 258, loss: 2.0600147247314453\n",
      "step: 259, loss: 1.7687158584594727\n",
      "step: 260, loss: 1.9908485412597656\n",
      "step: 261, loss: 1.9077163934707642\n",
      "step: 262, loss: 1.8688406944274902\n",
      "step: 263, loss: 1.827022671699524\n",
      "step: 264, loss: 1.8352131843566895\n",
      "step: 265, loss: 1.4831914901733398\n",
      "step: 266, loss: 2.134587287902832\n",
      "step: 267, loss: 1.8482879400253296\n",
      "step: 268, loss: 1.9748976230621338\n",
      "step: 269, loss: 2.023049831390381\n",
      "step: 270, loss: 1.6149667501449585\n",
      "step: 271, loss: 1.514370322227478\n",
      "step: 272, loss: 1.6230298280715942\n",
      "step: 273, loss: 1.5541367530822754\n",
      "step: 274, loss: 1.8802592754364014\n",
      "step: 275, loss: 1.919336199760437\n",
      "step: 276, loss: 1.377684473991394\n",
      "step: 277, loss: 1.0787286758422852\n",
      "step: 278, loss: 1.4705288410186768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 279, loss: 1.477145791053772\n",
      "step: 280, loss: 1.3441356420516968\n",
      "step: 281, loss: 1.2678834199905396\n",
      "step: 282, loss: 1.2775131464004517\n",
      "step: 283, loss: 1.7493427991867065\n",
      "step: 284, loss: 0.9106053113937378\n",
      "step: 285, loss: 1.5260062217712402\n",
      "step: 286, loss: 1.3682018518447876\n",
      "step: 287, loss: 1.4743714332580566\n",
      "step: 288, loss: 1.2332533597946167\n",
      "step: 289, loss: 1.3671822547912598\n",
      "step: 290, loss: 1.5977027416229248\n",
      "step: 291, loss: 1.6013449430465698\n",
      "step: 292, loss: 1.0792049169540405\n",
      "step: 293, loss: 1.3622686862945557\n",
      "step: 294, loss: 1.2012332677841187\n",
      "step: 295, loss: 1.1996619701385498\n",
      "step: 296, loss: 1.0319435596466064\n",
      "step: 297, loss: 1.0828254222869873\n",
      "step: 298, loss: 1.460838794708252\n",
      "step: 299, loss: 0.8668910264968872\n",
      "step: 300, loss: 0.6195905804634094\n",
      "step: 301, loss: 1.0267127752304077\n",
      "step: 302, loss: 1.3063524961471558\n",
      "step: 303, loss: 1.139296293258667\n",
      "step: 304, loss: 1.0466418266296387\n",
      "step: 305, loss: 0.9359961152076721\n",
      "step: 306, loss: 0.8217629790306091\n",
      "step: 307, loss: 1.2202337980270386\n",
      "step: 308, loss: 0.8232963681221008\n",
      "step: 309, loss: 0.9284637570381165\n",
      "step: 310, loss: 1.3492599725723267\n",
      "step: 311, loss: 0.8148993849754333\n",
      "step: 312, loss: 0.6079673767089844\n",
      "step: 313, loss: 1.0406630039215088\n",
      "step: 314, loss: 1.1818140745162964\n",
      "step: 315, loss: 1.4208422899246216\n",
      "step: 316, loss: 0.8984282612800598\n",
      "step: 317, loss: 0.8547302484512329\n",
      "step: 318, loss: 1.101984977722168\n",
      "step: 319, loss: 0.9030752778053284\n",
      "step: 320, loss: 0.8141511678695679\n",
      "step: 321, loss: 0.9733548164367676\n",
      "step: 322, loss: 0.7898085117340088\n",
      "step: 323, loss: 0.679127037525177\n",
      "step: 324, loss: 0.8010269999504089\n",
      "step: 325, loss: 0.6480002403259277\n",
      "step: 326, loss: 0.5900740623474121\n",
      "step: 327, loss: 0.9766747951507568\n",
      "step: 328, loss: 0.5544978976249695\n",
      "step: 329, loss: 1.1482009887695312\n",
      "step: 330, loss: 0.6987035274505615\n",
      "step: 331, loss: 0.7107298374176025\n",
      "step: 332, loss: 0.6704965233802795\n",
      "step: 333, loss: 0.9311668276786804\n",
      "step: 334, loss: 0.6055222153663635\n",
      "step: 335, loss: 0.802874743938446\n",
      "step: 336, loss: 0.9975684285163879\n",
      "step: 337, loss: 0.5019489526748657\n",
      "step: 338, loss: 1.0148898363113403\n",
      "step: 339, loss: 0.8015362620353699\n",
      "step: 340, loss: 0.7297685146331787\n",
      "step: 341, loss: 0.6548628807067871\n",
      "step: 342, loss: 0.6462103128433228\n",
      "step: 343, loss: 0.5819646716117859\n",
      "step: 344, loss: 0.6367460489273071\n",
      "step: 345, loss: 0.6454514265060425\n",
      "step: 346, loss: 0.8343165516853333\n",
      "step: 347, loss: 0.962786853313446\n",
      "step: 348, loss: 0.6112821102142334\n",
      "step: 349, loss: 0.9705641269683838\n",
      "step: 350, loss: 0.6238391399383545\n",
      "step: 351, loss: 0.4953151345252991\n",
      "step: 352, loss: 0.7225510478019714\n",
      "step: 353, loss: 0.874215841293335\n",
      "step: 354, loss: 0.8234397768974304\n",
      "step: 355, loss: 0.4381420910358429\n",
      "step: 356, loss: 0.7474812269210815\n",
      "step: 357, loss: 0.4452982246875763\n",
      "step: 358, loss: 0.6056357026100159\n",
      "step: 359, loss: 0.5538750886917114\n",
      "step: 360, loss: 0.4533979892730713\n",
      "step: 361, loss: 0.6558440923690796\n",
      "step: 362, loss: 0.8876017928123474\n",
      "step: 363, loss: 0.5127238631248474\n",
      "step: 364, loss: 0.7613592743873596\n",
      "step: 365, loss: 0.40210530161857605\n",
      "step: 366, loss: 0.426504909992218\n",
      "step: 367, loss: 0.6709074974060059\n",
      "step: 368, loss: 0.43867942690849304\n",
      "step: 369, loss: 0.446196585893631\n",
      "step: 370, loss: 0.5229653716087341\n",
      "step: 371, loss: 0.2445039004087448\n",
      "step: 372, loss: 0.3720642328262329\n",
      "step: 373, loss: 0.5781112313270569\n",
      "step: 374, loss: 0.4636191129684448\n",
      "step: 375, loss: 0.7193652391433716\n",
      "step: 376, loss: 0.6115608215332031\n",
      "step: 377, loss: 0.553541898727417\n",
      "step: 378, loss: 0.5971267223358154\n",
      "step: 379, loss: 0.17457698285579681\n",
      "step: 380, loss: 0.8494248390197754\n",
      "step: 381, loss: 0.5530599355697632\n",
      "step: 382, loss: 0.8653743267059326\n",
      "step: 383, loss: 0.5295324921607971\n",
      "step: 384, loss: 0.8463625311851501\n",
      "step: 385, loss: 0.47515586018562317\n",
      "step: 386, loss: 0.6609945893287659\n",
      "step: 387, loss: 0.39722976088523865\n",
      "step: 388, loss: 0.42508646845817566\n",
      "step: 389, loss: 0.5593486428260803\n",
      "step: 390, loss: 0.596977174282074\n",
      "step: 391, loss: 0.4578293561935425\n",
      "step: 392, loss: 0.42907384037971497\n",
      "step: 393, loss: 0.8155105710029602\n",
      "step: 394, loss: 0.6275002956390381\n",
      "step: 395, loss: 0.6475803256034851\n",
      "step: 396, loss: 0.5975666046142578\n",
      "step: 397, loss: 0.39116737246513367\n",
      "step: 398, loss: 0.2750037908554077\n",
      "step: 399, loss: 0.4369349181652069\n",
      "step: 400, loss: 0.7819763422012329\n",
      "step: 401, loss: 0.7305123209953308\n",
      "step: 402, loss: 0.2718990445137024\n",
      "step: 403, loss: 0.5764026641845703\n",
      "step: 404, loss: 0.33694010972976685\n",
      "step: 405, loss: 0.8589033484458923\n",
      "step: 406, loss: 0.5697361826896667\n",
      "step: 407, loss: 0.7813472151756287\n",
      "step: 408, loss: 0.9739677309989929\n",
      "step: 409, loss: 0.3795137405395508\n",
      "step: 410, loss: 0.23810644447803497\n",
      "step: 411, loss: 0.3186158239841461\n",
      "step: 412, loss: 0.5556831955909729\n",
      "step: 413, loss: 0.34882038831710815\n",
      "step: 414, loss: 0.6950985789299011\n",
      "step: 415, loss: 0.6238982677459717\n",
      "step: 416, loss: 0.554512619972229\n",
      "step: 417, loss: 0.30093157291412354\n",
      "step: 418, loss: 0.7229728102684021\n",
      "step: 419, loss: 0.49326232075691223\n",
      "step: 420, loss: 0.18323886394500732\n",
      "step: 421, loss: 0.22410184144973755\n",
      "step: 422, loss: 0.43855294585227966\n",
      "step: 423, loss: 0.5155083537101746\n",
      "step: 424, loss: 0.5240920782089233\n",
      "step: 425, loss: 0.2701028287410736\n",
      "step: 426, loss: 0.6799512505531311\n",
      "step: 427, loss: 0.46284201741218567\n",
      "step: 428, loss: 0.3800540864467621\n",
      "step: 429, loss: 0.3617590367794037\n",
      "step: 430, loss: 0.4950469136238098\n",
      "step: 431, loss: 0.4186278283596039\n",
      "step: 432, loss: 0.7060985565185547\n",
      "step: 433, loss: 0.10295020043849945\n",
      "step: 434, loss: 0.30504313111305237\n",
      "step: 435, loss: 0.40268123149871826\n",
      "step: 436, loss: 0.20801034569740295\n",
      "step: 437, loss: 0.5091110467910767\n",
      "step: 438, loss: 0.7885437607765198\n",
      "step: 439, loss: 0.624590277671814\n",
      "step: 440, loss: 0.39461982250213623\n",
      "step: 441, loss: 0.45797404646873474\n",
      "step: 442, loss: 0.4558645486831665\n",
      "step: 443, loss: 0.4997602701187134\n",
      "step: 444, loss: 0.33653053641319275\n",
      "step: 445, loss: 0.32666251063346863\n",
      "step: 446, loss: 0.41725078225135803\n",
      "step: 447, loss: 0.3902342915534973\n",
      "step: 448, loss: 0.6201018691062927\n",
      "step: 449, loss: 0.21606606245040894\n",
      "step: 450, loss: 0.27913063764572144\n",
      "step: 451, loss: 0.34310173988342285\n",
      "step: 452, loss: 0.3778996169567108\n",
      "step: 453, loss: 0.2970873713493347\n",
      "step: 454, loss: 0.18462827801704407\n",
      "step: 455, loss: 0.07608425617218018\n",
      "step: 456, loss: 0.1096479594707489\n",
      "step: 457, loss: 0.27619749307632446\n",
      "step: 458, loss: 0.09920215606689453\n",
      "step: 459, loss: 0.09409336745738983\n",
      "step: 460, loss: 0.1102365031838417\n",
      "step: 461, loss: 0.05321790277957916\n",
      "step: 462, loss: 0.3562085032463074\n",
      "step: 463, loss: 0.34722885489463806\n",
      "step: 464, loss: 0.2014501690864563\n",
      "step: 465, loss: 0.27474623918533325\n",
      "step: 466, loss: 0.167864590883255\n",
      "step: 467, loss: 0.20437483489513397\n",
      "step: 468, loss: 0.17198866605758667\n",
      "step: 469, loss: 0.401519238948822\n",
      "step: 470, loss: 0.09636950492858887\n",
      "step: 471, loss: 0.10970553755760193\n",
      "step: 472, loss: 0.03923630714416504\n",
      "step: 473, loss: 0.03434538468718529\n",
      "step: 474, loss: 0.24003718793392181\n",
      "step: 475, loss: 0.23180736601352692\n",
      "step: 476, loss: 0.1462457925081253\n",
      "step: 477, loss: 0.08609260618686676\n",
      "step: 478, loss: 0.12953363358974457\n",
      "step: 479, loss: 0.08943179249763489\n",
      "step: 480, loss: 0.16178825497627258\n",
      "step: 481, loss: 0.24287082254886627\n",
      "step: 482, loss: 0.0946001335978508\n",
      "step: 483, loss: 0.09036684036254883\n",
      "step: 484, loss: 0.1421073079109192\n",
      "step: 485, loss: 0.1856047660112381\n",
      "step: 486, loss: 0.1684483289718628\n",
      "step: 487, loss: 0.06922295689582825\n",
      "step: 488, loss: 0.08995084464550018\n",
      "step: 489, loss: 0.14454780519008636\n",
      "step: 490, loss: 0.19471679627895355\n",
      "step: 491, loss: 0.18456940352916718\n",
      "step: 492, loss: 0.29596269130706787\n",
      "step: 493, loss: 0.3618675172328949\n",
      "step: 494, loss: 0.2313184142112732\n",
      "step: 495, loss: 0.04938437417149544\n",
      "step: 496, loss: 0.21927018463611603\n",
      "step: 497, loss: 0.18353217840194702\n",
      "step: 498, loss: 0.32791027426719666\n",
      "step: 499, loss: 0.5235459208488464\n",
      "Epoch: 2\n",
      "Accuracy on test data: 0.7611524163568774\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"NUM_CLASSES = 20\\nwith open(os.getcwd() + '/20news-bydate/20news-full-words-idfs.txt') as f:\\n    vocab_size = len(f.read().splitlines())\\nmlp = MLP(vocab_size = vocab_size, hidden_size = 50)\\npredicted_labels, loss = mlp.build_graph()\\ntrain_op = mlp.trainer(loss = loss, learning_rate = 0.1)\\nwith tf.Session() as sess:\\n    train_data_reader, test_data_reader = load_dataset(vocab_size)\\n    max_step = 500\\n    sess.run(tf.global_variables_initializer())\\n    for step in range(max_step):\\n        train_data, train_labels = train_data_reader.next_batch()\\n        labels_eval, loss_eval, _ = sess.run(\\n            [predicted_labels, loss, train_op],\\n            feed_dict={\\n                mlp._X: train_data,\\n                mlp._real_Y: train_labels\\n            }\\n        )\\n        print('step: {}, loss: {}'.format(step, loss_eval))\\n        if (train_data_reader._batch_id == 0):\\n            trainable_variables = tf.trainable_variables()\\n            for variable in trainable_variables:\\n                save_parameters(\\n                name = variable.name,\\n                value = variable.eval(),\\n                epoch = train_data_reader._num_epoch\\n            )\\nwith tf.Session() as sess:\\n    epoch = train_data_reader._num_epoch\\n    trainable_variables = tf.trainable_variables()\\n    for variable in trainable_variables:\\n        saved_value = restore_parameters(variable.name,epoch)\\n        assign_op = variable.assign(saved_value)\\n        sess.run(assign_op)\\n    num_true_preds = 0\\n    while True:  \\n        test_data, test_labels = test_data_reader.next_batch()\\n        test_labels_eval = sess.run(\\n            predicted_labels,\\n            feed_dict = {\\n                mlp._X: test_data,\\n                mlp._real_Y: test_labels\\n            }\\n        )\\n        matches = np.equal(test_labels_eval, test_labels)\\n        num_true_preds += np.sum(matches.astype(float))\\n        if test_data_reader._batch_id == 0:\\n            break\\n    print('Epoch:', epoch)\\n    print('Accuracy on test data:', num_true_preds/len(test_data_reader._data))\";\n",
       "                var nbb_formatted_code = \"NUM_CLASSES = 20\\nwith open(os.getcwd() + \\\"/20news-bydate/20news-full-words-idfs.txt\\\") as f:\\n    vocab_size = len(f.read().splitlines())\\nmlp = MLP(vocab_size=vocab_size, hidden_size=50)\\npredicted_labels, loss = mlp.build_graph()\\ntrain_op = mlp.trainer(loss=loss, learning_rate=0.1)\\nwith tf.Session() as sess:\\n    train_data_reader, test_data_reader = load_dataset(vocab_size)\\n    max_step = 500\\n    sess.run(tf.global_variables_initializer())\\n    for step in range(max_step):\\n        train_data, train_labels = train_data_reader.next_batch()\\n        labels_eval, loss_eval, _ = sess.run(\\n            [predicted_labels, loss, train_op],\\n            feed_dict={mlp._X: train_data, mlp._real_Y: train_labels},\\n        )\\n        print(\\\"step: {}, loss: {}\\\".format(step, loss_eval))\\n        if train_data_reader._batch_id == 0:\\n            trainable_variables = tf.trainable_variables()\\n            for variable in trainable_variables:\\n                save_parameters(\\n                    name=variable.name,\\n                    value=variable.eval(),\\n                    epoch=train_data_reader._num_epoch,\\n                )\\nwith tf.Session() as sess:\\n    epoch = train_data_reader._num_epoch\\n    trainable_variables = tf.trainable_variables()\\n    for variable in trainable_variables:\\n        saved_value = restore_parameters(variable.name, epoch)\\n        assign_op = variable.assign(saved_value)\\n        sess.run(assign_op)\\n    num_true_preds = 0\\n    while True:\\n        test_data, test_labels = test_data_reader.next_batch()\\n        test_labels_eval = sess.run(\\n            predicted_labels, feed_dict={mlp._X: test_data, mlp._real_Y: test_labels}\\n        )\\n        matches = np.equal(test_labels_eval, test_labels)\\n        num_true_preds += np.sum(matches.astype(float))\\n        if test_data_reader._batch_id == 0:\\n            break\\n    print(\\\"Epoch:\\\", epoch)\\n    print(\\\"Accuracy on test data:\\\", num_true_preds / len(test_data_reader._data))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_CLASSES = 20\n",
    "with open(os.getcwd() + '/20news-bydate/20news-full-words-idfs.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "mlp = MLP(vocab_size = vocab_size, hidden_size = 50)\n",
    "predicted_labels, loss = mlp.build_graph()\n",
    "train_op = mlp.trainer(loss = loss, learning_rate = 0.1)\n",
    "with tf.Session() as sess:\n",
    "    train_data_reader, test_data_reader = load_dataset(vocab_size)\n",
    "    max_step = 500\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(max_step):\n",
    "        train_data, train_labels = train_data_reader.next_batch()\n",
    "        labels_eval, loss_eval, _ = sess.run(\n",
    "            [predicted_labels, loss, train_op],\n",
    "            feed_dict={\n",
    "                mlp._X: train_data,\n",
    "                mlp._real_Y: train_labels\n",
    "            }\n",
    "        )\n",
    "        print('step: {}, loss: {}'.format(step, loss_eval))\n",
    "        if (train_data_reader._batch_id == 0):\n",
    "            trainable_variables = tf.trainable_variables()\n",
    "            for variable in trainable_variables:\n",
    "                save_parameters(\n",
    "                name = variable.name,\n",
    "                value = variable.eval(),\n",
    "                epoch = train_data_reader._num_epoch\n",
    "            )\n",
    "with tf.Session() as sess:\n",
    "    epoch = train_data_reader._num_epoch\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saved_value = restore_parameters(variable.name,epoch)\n",
    "        assign_op = variable.assign(saved_value)\n",
    "        sess.run(assign_op)\n",
    "    num_true_preds = 0\n",
    "    while True:  \n",
    "        test_data, test_labels = test_data_reader.next_batch()\n",
    "        test_labels_eval = sess.run(\n",
    "            predicted_labels,\n",
    "            feed_dict = {\n",
    "                mlp._X: test_data,\n",
    "                mlp._real_Y: test_labels\n",
    "            }\n",
    "        )\n",
    "        matches = np.equal(test_labels_eval, test_labels)\n",
    "        num_true_preds += np.sum(matches.astype(float))\n",
    "        if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "    print('Epoch:', epoch)\n",
    "    print('Accuracy on test data:', num_true_preds/len(test_data_reader._data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
