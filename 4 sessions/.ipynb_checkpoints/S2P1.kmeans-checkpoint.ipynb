{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbafb405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43574893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Member:\n",
    "    def __init__(self, r_d, label=None, doc_id=None):\n",
    "        self._r_d = r_d\n",
    "        self._label = label\n",
    "        self._doc_id = doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea78879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self):\n",
    "        self._centroid = None\n",
    "        self._members = []\n",
    "\n",
    "    def reset_members(self):\n",
    "        self._members = []\n",
    "\n",
    "    def add_member(self, member):\n",
    "        self._members.append(member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e89a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    def __init__(self, num_clusters):\n",
    "        self._num_clusters = num_clusters\n",
    "        self._clusters = [Cluster() for _ in range(self._num_clusters)]\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        def sparse_to_dense(sparse_r_d, vocab_size):\n",
    "            r_d = [0.0 for _ in range(vocab_size)]\n",
    "            indices_tfidfs = sparse_r_d.split()\n",
    "            for index_tfidf in indices_tfidfs:\n",
    "                index = int(index_tfidf.split(\":\")[0])\n",
    "                tfidf = float(index_tfidf.split(\":\")[1])\n",
    "                r_d[index] = tfidf\n",
    "            return np.array(r_d)\n",
    "\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "\n",
    "        with open(os.getcwd()+\"/20news-bydate/20news-full-words-idfs.txt\") as f:\n",
    "            vocab_size = len(f.read().splitlines())\n",
    "\n",
    "        self._data = []\n",
    "        self._label_count = defaultdict(int)\n",
    "        for d in d_lines:\n",
    "            features = d.split(\"<fff>\")\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            if label not in self._label_count:\n",
    "                print(f\"Loading cluster {label}\")\n",
    "            self._label_count[label] += 1\n",
    "            r_d = sparse_to_dense(sparse_r_d=features[2], vocab_size=vocab_size)\n",
    "            self._data.append(Member(r_d=r_d, label=label, doc_id=doc_id))\n",
    "\n",
    "    def compute_similarity(self, a, b):\n",
    "        return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "    def random_init(self, seed_value_id):\n",
    "        start_time = time()\n",
    "        set_candidates = set(range(len(self._data)))\n",
    "        set_candidates.remove(seed_value_id)\n",
    "        print(f\"Seed centroid: {seed_value_id}\")\n",
    "        E = [self._data[seed_value_id]._r_d]\n",
    "        while len(E) < self._num_clusters:\n",
    "            new_centroid_id = None\n",
    "            min_similarity_val = 1\n",
    "            for i in set_candidates:\n",
    "                local_max_similarity = -1\n",
    "                for centroid in E:\n",
    "                    local_max_similarity = max(local_max_similarity, self.compute_similarity(self._data[i]._r_d, centroid))\n",
    "                if local_max_similarity < min_similarity_val:\n",
    "                    new_centroid_id = i\n",
    "                    min_similarity_val = local_max_similarity\n",
    "            if new_centroid_id:\n",
    "                print(f\"Got new centroid {len(E)}: {new_centroid_id}\")\n",
    "                set_candidates.remove(new_centroid_id)\n",
    "                E.append(self._data[new_centroid_id]._r_d)\n",
    "            else:\n",
    "                raise Exeption(\"Could not find enough centroids\")\n",
    "        for i in range(len(self._clusters)):\n",
    "            self._clusters[i]._centroid = E[i]\n",
    "        end_time = time()\n",
    "        print(\"Execution time: \", end_time - start_time)\n",
    "\n",
    "    def select_cluster_for(self, member):\n",
    "        best_fit_cluster = None\n",
    "        max_similarity = -1\n",
    "        for cluster in self._clusters:\n",
    "            similarity = self.compute_similarity(member._r_d, cluster._centroid)\n",
    "            if similarity > max_similarity:\n",
    "                best_fit_cluster = cluster\n",
    "                max_similarity = similarity\n",
    "        best_fit_cluster.add_member(member)\n",
    "        return max_similarity\n",
    "\n",
    "    def update_centroid_of(self, cluster):\n",
    "        aver_r_d = np.mean([member._r_d for member in cluster._members], axis=0)\n",
    "        new_centroid = aver_r_d/np.linalg.norm(aver_r_d)\n",
    "        cluster._centroid = new_centroid\n",
    "        return cluster._centroid\n",
    "\n",
    "    def stopping_condition(self, criterion, threshold):\n",
    "        criteria = [\"max_iters\", \"centroid\", \"similarity\"]\n",
    "        assert criterion in criteria\n",
    "        if criterion == \"max_iters\":\n",
    "            print(f\"Iterations: {self._iteration}\")\n",
    "            if self._iteration >= threshold:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        elif criterion == \"centroid\":        \n",
    "            E_new_minus_E = [centroid for centroid in self._E_new if centroid not in self._E]\n",
    "            print(f\"Number centroid changed: {len(E_new_minus_E)}\")\n",
    "            if len(E_new_minus_E) <= threshold:\n",
    "                return True\n",
    "            return False\n",
    "            \n",
    "        else:\n",
    "            S_new_minus_S = self._S_new - self._S\n",
    "            print(f\"S changed: {S_new_minus_S}\\tOld S: {self._S}\\tNew S: {self._S_new}\")\n",
    "            if abs(S_new_minus_S) <= threshold:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def run(self, seed_value_id, criterion, threshold):\n",
    "        self.random_init(seed_value_id)\n",
    "        self._iteration = 0\n",
    "        self._S = 0\n",
    "        self._E = []\n",
    "\n",
    "        while True:\n",
    "            self._iteration += 1\n",
    "            print(\"Processing iteration:\", self._iteration)\n",
    "            for cluster in self._clusters:\n",
    "                cluster.reset_members()\n",
    "            \n",
    "            self._S_new = 0\n",
    "            for member in self._data:\n",
    "                self._S_new += self.select_cluster_for(member)\n",
    "\n",
    "            self._E_new = []\n",
    "            for cluster in self._clusters:\n",
    "                self._E_new.append(list(self.update_centroid_of(cluster)))\n",
    "\n",
    "            if self.stopping_condition(criterion, threshold):\n",
    "                print(\"Catch stopping condition by criterion:\", criterion)\n",
    "                break\n",
    "\n",
    "            self._S = self._S_new\n",
    "            self._E = self._E_new\n",
    "\n",
    "    def compute_purity(self, num_clusters):\n",
    "        majority_sum = 0\n",
    "        for cluster in self._clusters:\n",
    "            member_labels = [member._label for member in cluster._members]\n",
    "            max_count = max([member_labels.count(label) for label in range(num_clusters)])\n",
    "            majority_sum += max_count\n",
    "        return majority_sum * 1 / len(self._data)\n",
    "\n",
    "    def compute_NMI(self, num_clusters):\n",
    "        I_value, H_O, H_C, N = 0.0, 0.0, 0.0, len(self._data)\n",
    "\n",
    "        for cluster in self._clusters:\n",
    "            wk = len(cluster._members) * 1.0\n",
    "            H_O += -wk / N * np.log10(wk / N)\n",
    "\n",
    "        for label in range(num_clusters):\n",
    "            cj = self._label_count[label] * 1.0\n",
    "            H_C += -cj / N * np.log10(cj / N)\n",
    "\n",
    "        for cluster in self._clusters:\n",
    "            member_labels = [member._label for member in cluster._members]\n",
    "            for label in range(num_clusters):\n",
    "                wk_cj = member_labels.count(label) * 1.0\n",
    "                wk = len(cluster._members) * 1.0\n",
    "                cj = self._label_count[label] * 1.0\n",
    "                I_value += wk_cj / N * np.log10(N * wk_cj / (wk * cj) + 1e-12)\n",
    "\n",
    "        return I_value * 2.0 / (H_C + H_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b045597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kmeans sample\n",
    "num_clusters = 20\n",
    "k = Kmeans(num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7809077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cluster 0\n",
      "Loading cluster 1\n",
      "Loading cluster 2\n",
      "Loading cluster 3\n",
      "Loading cluster 4\n",
      "Loading cluster 5\n",
      "Loading cluster 6\n",
      "Loading cluster 7\n",
      "Loading cluster 8\n",
      "Loading cluster 9\n",
      "Loading cluster 10\n",
      "Loading cluster 11\n",
      "Loading cluster 12\n",
      "Loading cluster 13\n",
      "Loading cluster 14\n",
      "Loading cluster 15\n",
      "Loading cluster 16\n",
      "Loading cluster 17\n",
      "Loading cluster 18\n",
      "Loading cluster 19\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "k.load_data(os.getcwd()+\"/20news-bydate/20news-full-tf-idf.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4306335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given number clusters: 20\n",
      "Number clusters: 20\n",
      "Label dictionary: defaultdict(<class 'int'>, {0: 799, 1: 973, 2: 985, 3: 982, 4: 963, 5: 988, 6: 975, 7: 990, 8: 996, 9: 994, 10: 999, 11: 991, 12: 984, 13: 990, 14: 987, 15: 997, 16: 910, 17: 940, 18: 775, 19: 628})\n"
     ]
    }
   ],
   "source": [
    "# Clusters \n",
    "print(f\"Given number clusters: {num_clusters}\")\n",
    "print(f\"Number clusters: {len(k._label_count)}\")\n",
    "print(f\"Label dictionary: {k._label_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46843466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed centroid: 10053\n",
      "Got new centroid 1: 16894\n",
      "Got new centroid 2: 5454\n",
      "Got new centroid 3: 1553\n",
      "Got new centroid 4: 9968\n",
      "Got new centroid 5: 3881\n",
      "Got new centroid 6: 1342\n",
      "Got new centroid 7: 1275\n",
      "Got new centroid 8: 832\n",
      "Got new centroid 9: 7778\n",
      "Got new centroid 10: 2339\n",
      "Got new centroid 11: 13605\n",
      "Got new centroid 12: 2175\n",
      "Got new centroid 13: 15126\n",
      "Got new centroid 14: 16775\n",
      "Got new centroid 15: 8041\n",
      "Got new centroid 16: 15321\n",
      "Got new centroid 17: 3896\n",
      "Got new centroid 18: 10470\n",
      "Got new centroid 19: 6737\n",
      "Execution time:  62.81085968017578\n",
      "Processing iteration: 1\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 2\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 3\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 4\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 5\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 6\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 7\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 8\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 9\n",
      "Number centroid changed: 19\n",
      "Processing iteration: 10\n",
      "Number centroid changed: 18\n",
      "Processing iteration: 11\n",
      "Number centroid changed: 19\n",
      "Processing iteration: 12\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 13\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 14\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 15\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 16\n",
      "Number centroid changed: 18\n",
      "Processing iteration: 17\n",
      "Number centroid changed: 19\n",
      "Processing iteration: 18\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 19\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 20\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 21\n",
      "Number centroid changed: 19\n",
      "Processing iteration: 22\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 23\n",
      "Number centroid changed: 20\n",
      "Processing iteration: 24\n",
      "Number centroid changed: 17\n",
      "Processing iteration: 25\n",
      "Number centroid changed: 18\n",
      "Processing iteration: 26\n",
      "Number centroid changed: 16\n",
      "Processing iteration: 27\n",
      "Number centroid changed: 11\n",
      "Processing iteration: 28\n",
      "Number centroid changed: 8\n",
      "Processing iteration: 29\n",
      "Number centroid changed: 5\n",
      "Processing iteration: 30\n",
      "Number centroid changed: 5\n",
      "Processing iteration: 31\n",
      "Number centroid changed: 7\n",
      "Processing iteration: 32\n",
      "Number centroid changed: 7\n",
      "Processing iteration: 33\n",
      "Number centroid changed: 7\n",
      "Processing iteration: 34\n",
      "Number centroid changed: 10\n",
      "Processing iteration: 35\n",
      "Number centroid changed: 11\n",
      "Processing iteration: 36\n",
      "Number centroid changed: 9\n",
      "Processing iteration: 37\n",
      "Number centroid changed: 5\n",
      "Processing iteration: 38\n",
      "Number centroid changed: 6\n",
      "Processing iteration: 39\n",
      "Number centroid changed: 8\n",
      "Processing iteration: 40\n",
      "Number centroid changed: 6\n",
      "Processing iteration: 41\n",
      "Number centroid changed: 5\n",
      "Processing iteration: 42\n",
      "Number centroid changed: 5\n",
      "Processing iteration: 43\n",
      "Number centroid changed: 4\n",
      "Processing iteration: 44\n",
      "Number centroid changed: 2\n",
      "Processing iteration: 45\n",
      "Number centroid changed: 0\n",
      "Catch stopping condition by criterion: centroid\n"
     ]
    }
   ],
   "source": [
    "# Running\n",
    "k.run(seed_value_id=random.choice(range(len(k._data))), criterion=\"centroid\", threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c26ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.49877958187413773\n",
      "NMI: 0.5286004130347571\n"
     ]
    }
   ],
   "source": [
    "# Measurements\n",
    "print(f\"Purity: {k.compute_purity(num_clusters)}\")\n",
    "print(f\"NMI: {k.compute_NMI(num_clusters)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
