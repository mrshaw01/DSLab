{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e8a482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9f9a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import os\\nimport sys\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow.compat.v1 as tf\\n\\ntf.disable_v2_behavior()\";\n",
       "                var nbb_formatted_code = \"import os\\nimport sys\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow.compat.v1 as tf\\n\\ntf.disable_v2_behavior()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccbea12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"class MLP:\\n    def __init__(self, vocab_size, hidden_size):\\n        self._vocab_size = vocab_size\\n        self._hidden_size = hidden_size\\n\\n    def build_graph(self):\\n        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\\n        self._real_Y = tf.placeholder(\\n            tf.int32,\\n            shape=[\\n                None,\\n            ],\\n        )\\n        weights_1 = tf.get_variable(\\n            name=\\\"weight_input_hidden\\\",\\n            shape=(self._vocab_size, self._hidden_size),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n        biases_1 = tf.get_variable(\\n            name=\\\"biases_input_hidden\\\",\\n            shape=(self._hidden_size),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        weights_2 = tf.get_variable(\\n            name=\\\"weights_hidden_output\\\",\\n            shape=(self._hidden_size, NUM_CLASSES),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        biases_2 = tf.get_variable(\\n            name=\\\"biases_hidden_output\\\",\\n            shape=(NUM_CLASSES),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        hidden = tf.matmul(self._X, weights_1) + biases_1  # Net input\\n        hidden = tf.sigmoid(hidden)  # activation function.\\n\\n        logits = tf.matmul(hidden, weights_2) + biases_2\\n\\n        labels_one_hot = tf.one_hot(\\n            indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32\\n        )\\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\\n            labels=labels_one_hot, logits=logits\\n        )\\n        loss = tf.reduce_mean(loss)\\n        probs = tf.nn.softmax(logits)  # difference between 2 distributions.\\n        predicted_labels = tf.argmax(probs, axis=1)\\n        predicted_labels = tf.squeeze(predicted_labels)\\n        return predicted_labels, loss\\n\\n    def trainer(self, loss, learning_rate):\\n        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\\n        return train_op\";\n",
       "                var nbb_formatted_code = \"class MLP:\\n    def __init__(self, vocab_size, hidden_size):\\n        self._vocab_size = vocab_size\\n        self._hidden_size = hidden_size\\n\\n    def build_graph(self):\\n        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\\n        self._real_Y = tf.placeholder(\\n            tf.int32,\\n            shape=[\\n                None,\\n            ],\\n        )\\n        weights_1 = tf.get_variable(\\n            name=\\\"weight_input_hidden\\\",\\n            shape=(self._vocab_size, self._hidden_size),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n        biases_1 = tf.get_variable(\\n            name=\\\"biases_input_hidden\\\",\\n            shape=(self._hidden_size),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        weights_2 = tf.get_variable(\\n            name=\\\"weights_hidden_output\\\",\\n            shape=(self._hidden_size, NUM_CLASSES),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        biases_2 = tf.get_variable(\\n            name=\\\"biases_hidden_output\\\",\\n            shape=(NUM_CLASSES),\\n            initializer=tf.random_normal_initializer(seed=2021),\\n        )\\n\\n        hidden = tf.matmul(self._X, weights_1) + biases_1  # Net input\\n        hidden = tf.sigmoid(hidden)  # activation function.\\n\\n        logits = tf.matmul(hidden, weights_2) + biases_2\\n\\n        labels_one_hot = tf.one_hot(\\n            indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32\\n        )\\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\\n            labels=labels_one_hot, logits=logits\\n        )\\n        loss = tf.reduce_mean(loss)\\n        probs = tf.nn.softmax(logits)  # difference between 2 distributions.\\n        predicted_labels = tf.argmax(probs, axis=1)\\n        predicted_labels = tf.squeeze(predicted_labels)\\n        return predicted_labels, loss\\n\\n    def trainer(self, loss, learning_rate):\\n        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\\n        return train_op\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._hidden_size = hidden_size\n",
    "\n",
    "    def build_graph(self):\n",
    "        self._X = tf.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
    "        self._real_Y = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[\n",
    "                None,\n",
    "            ],\n",
    "        )\n",
    "        weights_1 = tf.get_variable(\n",
    "            name=\"weight_input_hidden\",\n",
    "            shape=(self._vocab_size, self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "        biases_1 = tf.get_variable(\n",
    "            name=\"biases_input_hidden\",\n",
    "            shape=(self._hidden_size),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "\n",
    "        weights_2 = tf.get_variable(\n",
    "            name=\"weights_hidden_output\",\n",
    "            shape=(self._hidden_size, NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "\n",
    "        biases_2 = tf.get_variable(\n",
    "            name=\"biases_hidden_output\",\n",
    "            shape=(NUM_CLASSES),\n",
    "            initializer=tf.random_normal_initializer(seed=2021),\n",
    "        )\n",
    "\n",
    "        hidden = tf.matmul(self._X, weights_1) + biases_1  # Net input\n",
    "        hidden = tf.sigmoid(hidden)  # activation function.\n",
    "\n",
    "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "\n",
    "        labels_one_hot = tf.one_hot(\n",
    "            indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32\n",
    "        )\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=labels_one_hot, logits=logits\n",
    "        )\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        probs = tf.nn.softmax(logits)  # difference between 2 distributions.\n",
    "        predicted_labels = tf.argmax(probs, axis=1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        return predicted_labels, loss\n",
    "\n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005069b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class DataReader:\\n    def __init__(self, data_path, batch_size, vocab_size):\\n        self._batch_size = batch_size\\n        with open(data_path) as f:\\n            d_lines = f.read().splitlines()\\n\\n        self._data = []\\n        self._labels = []\\n        for line in d_lines:\\n            vector = [0.0 for _ in range(vocab_size)]\\n            features = line.split(\\\"<fff>\\\")\\n            label, doc_id = int(features[0]), int(features[1])\\n            tokens = features[2].split()\\n            for token in tokens:\\n                index, value = int(token.split(\\\":\\\")[0]), float(token.split(\\\":\\\")[1])\\n                vector[index] = value\\n            self._data.append(vector)\\n            self._labels.append(label)\\n\\n        self._data = np.array(self._data)\\n        self._labels = np.array(self._labels)\\n        self._num_epoch = 0\\n        self._batch_id = 0\\n\\n    def next_batch(self):\\n        start = self._batch_id * self._batch_size\\n        end = min(start + self._batch_size, len(self._data))\\n        self._batch_id += 1\\n\\n        if end == len(self._data):\\n            self._num_epoch += 1\\n            self._batch_id = 0\\n            indices = list(range(len(self._data)))\\n            random.seed(2021)\\n            random.shuffle(indices)\\n            self._data, self._labels = self._data[indices], self._labels[indices]\\n        return self._data[start:end], self._labels[start:end]\";\n",
       "                var nbb_formatted_code = \"class DataReader:\\n    def __init__(self, data_path, batch_size, vocab_size):\\n        self._batch_size = batch_size\\n        with open(data_path) as f:\\n            d_lines = f.read().splitlines()\\n\\n        self._data = []\\n        self._labels = []\\n        for line in d_lines:\\n            vector = [0.0 for _ in range(vocab_size)]\\n            features = line.split(\\\"<fff>\\\")\\n            label, doc_id = int(features[0]), int(features[1])\\n            tokens = features[2].split()\\n            for token in tokens:\\n                index, value = int(token.split(\\\":\\\")[0]), float(token.split(\\\":\\\")[1])\\n                vector[index] = value\\n            self._data.append(vector)\\n            self._labels.append(label)\\n\\n        self._data = np.array(self._data)\\n        self._labels = np.array(self._labels)\\n        self._num_epoch = 0\\n        self._batch_id = 0\\n\\n    def next_batch(self):\\n        start = self._batch_id * self._batch_size\\n        end = min(start + self._batch_size, len(self._data))\\n        self._batch_id += 1\\n\\n        if end == len(self._data):\\n            self._num_epoch += 1\\n            self._batch_id = 0\\n            indices = list(range(len(self._data)))\\n            random.seed(2021)\\n            random.shuffle(indices)\\n            self._data, self._labels = self._data[indices], self._labels[indices]\\n        return self._data[start:end], self._labels[start:end]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data_path, batch_size, vocab_size):\n",
    "        self._batch_size = batch_size\n",
    "        with open(data_path) as f:\n",
    "            d_lines = f.read().splitlines()\n",
    "\n",
    "        self._data = []\n",
    "        self._labels = []\n",
    "        for line in d_lines:\n",
    "            vector = [0.0 for _ in range(vocab_size)]\n",
    "            features = line.split(\"<fff>\")\n",
    "            label, doc_id = int(features[0]), int(features[1])\n",
    "            tokens = features[2].split()\n",
    "            for token in tokens:\n",
    "                index, value = int(token.split(\":\")[0]), float(token.split(\":\")[1])\n",
    "                vector[index] = value\n",
    "            self._data.append(vector)\n",
    "            self._labels.append(label)\n",
    "\n",
    "        self._data = np.array(self._data)\n",
    "        self._labels = np.array(self._labels)\n",
    "        self._num_epoch = 0\n",
    "        self._batch_id = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        start = self._batch_id * self._batch_size\n",
    "        end = min(start + self._batch_size, len(self._data))\n",
    "        self._batch_id += 1\n",
    "\n",
    "        if end == len(self._data):\n",
    "            self._num_epoch += 1\n",
    "            self._batch_id = 0\n",
    "            indices = list(range(len(self._data)))\n",
    "            random.seed(2021)\n",
    "            random.shuffle(indices)\n",
    "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "        return self._data[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de0ddc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def load_dataset(vocab_size):\\n    train_data_reader = DataReader(\\n        data_path=os.getcwd() + \\\"/20news-bydate/20news-train-tf-idf.txt\\\",\\n        batch_size=50,\\n        vocab_size=vocab_size,\\n    )\\n\\n    test_data_reader = DataReader(\\n        data_path=os.getcwd() + \\\"/20news-bydate/20news-test-tf-idf.txt\\\",\\n        batch_size=50,\\n        vocab_size=vocab_size,\\n    )\\n\\n    return train_data_reader, test_data_reader\";\n",
       "                var nbb_formatted_code = \"def load_dataset(vocab_size):\\n    train_data_reader = DataReader(\\n        data_path=os.getcwd() + \\\"/20news-bydate/20news-train-tf-idf.txt\\\",\\n        batch_size=50,\\n        vocab_size=vocab_size,\\n    )\\n\\n    test_data_reader = DataReader(\\n        data_path=os.getcwd() + \\\"/20news-bydate/20news-test-tf-idf.txt\\\",\\n        batch_size=50,\\n        vocab_size=vocab_size,\\n    )\\n\\n    return train_data_reader, test_data_reader\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_dataset(vocab_size):\n",
    "    train_data_reader = DataReader(\n",
    "        data_path=os.getcwd() + \"/20news-bydate/20news-train-tf-idf.txt\",\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size,\n",
    "    )\n",
    "\n",
    "    test_data_reader = DataReader(\n",
    "        data_path=os.getcwd() + \"/20news-bydate/20news-test-tf-idf.txt\",\n",
    "        batch_size=50,\n",
    "        vocab_size=vocab_size,\n",
    "    )\n",
    "\n",
    "    return train_data_reader, test_data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5854d24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"def save_parameters(name, value, epoch):\\n    filename = name.replace(\\\":\\\", \\\"-colon-\\\") + \\\"-epoch-{}.txt\\\".format(epoch)\\n    if len(value.shape) == 1:\\n        string_form = \\\",\\\".join([str(number) for number in value])\\n    else:\\n        string_form = \\\"\\\\n\\\".join(\\n            [\\n                \\\",\\\".join([str(number) for number in value[row]])\\n                for row in range(value.shape[0])\\n            ]\\n        )\\n    with open(os.getcwd() + \\\"/saved-paras/\\\" + filename, \\\"w\\\") as f:\\n        f.write(string_form)\";\n",
       "                var nbb_formatted_code = \"def save_parameters(name, value, epoch):\\n    filename = name.replace(\\\":\\\", \\\"-colon-\\\") + \\\"-epoch-{}.txt\\\".format(epoch)\\n    if len(value.shape) == 1:\\n        string_form = \\\",\\\".join([str(number) for number in value])\\n    else:\\n        string_form = \\\"\\\\n\\\".join(\\n            [\\n                \\\",\\\".join([str(number) for number in value[row]])\\n                for row in range(value.shape[0])\\n            ]\\n        )\\n    with open(os.getcwd() + \\\"/saved-paras/\\\" + filename, \\\"w\\\") as f:\\n        f.write(string_form)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_parameters(name, value, epoch):\n",
    "    filename = name.replace(\":\", \"-colon-\") + \"-epoch-{}.txt\".format(epoch)\n",
    "    if len(value.shape) == 1:\n",
    "        string_form = \",\".join([str(number) for number in value])\n",
    "    else:\n",
    "        string_form = \"\\n\".join(\n",
    "            [\n",
    "                \",\".join([str(number) for number in value[row]])\n",
    "                for row in range(value.shape[0])\n",
    "            ]\n",
    "        )\n",
    "    with open(os.getcwd() + \"/saved-paras/\" + filename, \"w\") as f:\n",
    "        f.write(string_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231c1bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def restore_parameters(name, epoch):\\n    filename = name.replace(\\\":\\\", \\\"-colon-\\\") + \\\"-epoch-{}.txt\\\".format(epoch)\\n    with open(os.getcwd() + \\\"/saved-paras/\\\" + filename, \\\"r\\\") as f:\\n        lines = f.read().splitlines()\\n    if len(lines) == 1:\\n        value = [float(number) for number in lines[0].split(\\\",\\\")]\\n    else:\\n        value = [\\n            [float(number) for number in lines[row].split(\\\",\\\")]\\n            for row in range(len(lines))\\n        ]\\n    return value\";\n",
       "                var nbb_formatted_code = \"def restore_parameters(name, epoch):\\n    filename = name.replace(\\\":\\\", \\\"-colon-\\\") + \\\"-epoch-{}.txt\\\".format(epoch)\\n    with open(os.getcwd() + \\\"/saved-paras/\\\" + filename, \\\"r\\\") as f:\\n        lines = f.read().splitlines()\\n    if len(lines) == 1:\\n        value = [float(number) for number in lines[0].split(\\\",\\\")]\\n    else:\\n        value = [\\n            [float(number) for number in lines[row].split(\\\",\\\")]\\n            for row in range(len(lines))\\n        ]\\n    return value\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def restore_parameters(name, epoch):\n",
    "    filename = name.replace(\":\", \"-colon-\") + \"-epoch-{}.txt\".format(epoch)\n",
    "    with open(os.getcwd() + \"/saved-paras/\" + filename, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "        value = [float(number) for number in lines[0].split(\",\")]\n",
    "    else:\n",
    "        value = [\n",
    "            [float(number) for number in lines[row].split(\",\")]\n",
    "            for row in range(len(lines))\n",
    "        ]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "967f94ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 13:38:26.506065: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 11.104410171508789\n",
      "step: 1, loss: 0.9813809990882874\n",
      "step: 2, loss: 0.0006089996313676238\n",
      "step: 3, loss: 2.036018940998474e-06\n",
      "step: 4, loss: 1.907348234908568e-08\n",
      "step: 5, loss: 0.0\n",
      "step: 6, loss: 0.0\n",
      "step: 7, loss: 0.0\n",
      "step: 8, loss: 0.0\n",
      "step: 9, loss: 14.782851219177246\n",
      "step: 10, loss: 35.11439514160156\n",
      "step: 11, loss: 31.772096633911133\n",
      "step: 12, loss: 26.245704650878906\n",
      "step: 13, loss: 20.384763717651367\n",
      "step: 14, loss: 12.772749900817871\n",
      "step: 15, loss: 6.130094528198242\n",
      "step: 16, loss: 0.990744948387146\n",
      "step: 17, loss: 0.06624089926481247\n",
      "step: 18, loss: 0.0001084230825654231\n",
      "step: 19, loss: 1.053785354088177e-06\n",
      "step: 20, loss: 0.0\n",
      "step: 21, loss: 24.634183883666992\n",
      "step: 22, loss: 32.63716506958008\n",
      "step: 23, loss: 29.58038902282715\n",
      "step: 24, loss: 26.681072235107422\n",
      "step: 25, loss: 22.07748794555664\n",
      "step: 26, loss: 17.576826095581055\n",
      "step: 27, loss: 12.419458389282227\n",
      "step: 28, loss: 8.495309829711914\n",
      "step: 29, loss: 4.281155109405518\n",
      "step: 30, loss: 1.077122688293457\n",
      "step: 31, loss: 0.046487484127283096\n",
      "step: 32, loss: 0.06662814319133759\n",
      "step: 33, loss: 17.661041259765625\n",
      "step: 34, loss: 19.809751510620117\n",
      "step: 35, loss: 18.38697052001953\n",
      "step: 36, loss: 16.343286514282227\n",
      "step: 37, loss: 14.193692207336426\n",
      "step: 38, loss: 11.481032371520996\n",
      "step: 39, loss: 8.820477485656738\n",
      "step: 40, loss: 6.483560562133789\n",
      "step: 41, loss: 4.291399002075195\n",
      "step: 42, loss: 2.39984130859375\n",
      "step: 43, loss: 0.8817689418792725\n",
      "step: 44, loss: 1.7890151739120483\n",
      "step: 45, loss: 15.761369705200195\n",
      "step: 46, loss: 14.412138938903809\n",
      "step: 47, loss: 14.547990798950195\n",
      "step: 48, loss: 12.690193176269531\n",
      "step: 49, loss: 11.249582290649414\n",
      "step: 50, loss: 9.529751777648926\n",
      "step: 51, loss: 7.860347747802734\n",
      "step: 52, loss: 6.282916069030762\n",
      "step: 53, loss: 4.623212814331055\n",
      "step: 54, loss: 3.624711275100708\n",
      "step: 55, loss: 2.379946231842041\n",
      "step: 56, loss: 5.7982707023620605\n",
      "step: 57, loss: 7.969674110412598\n",
      "step: 58, loss: 7.420160293579102\n",
      "step: 59, loss: 6.6519951820373535\n",
      "step: 60, loss: 5.669539928436279\n",
      "step: 61, loss: 4.632778167724609\n",
      "step: 62, loss: 3.725177526473999\n",
      "step: 63, loss: 2.813662528991699\n",
      "step: 64, loss: 2.026445150375366\n",
      "step: 65, loss: 1.4851789474487305\n",
      "step: 66, loss: 0.9275364875793457\n",
      "step: 67, loss: 0.5142601132392883\n",
      "step: 68, loss: 6.940255641937256\n",
      "step: 69, loss: 10.02556324005127\n",
      "step: 70, loss: 9.5852689743042\n",
      "step: 71, loss: 9.182806015014648\n",
      "step: 72, loss: 8.593782424926758\n",
      "step: 73, loss: 7.833108901977539\n",
      "step: 74, loss: 7.125202178955078\n",
      "step: 75, loss: 6.336867809295654\n",
      "step: 76, loss: 5.343020439147949\n",
      "step: 77, loss: 4.796245098114014\n",
      "step: 78, loss: 3.8779659271240234\n",
      "step: 79, loss: 3.3723254203796387\n",
      "step: 80, loss: 8.288185119628906\n",
      "step: 81, loss: 7.360990047454834\n",
      "step: 82, loss: 6.262777328491211\n",
      "step: 83, loss: 5.734777927398682\n",
      "step: 84, loss: 5.106266498565674\n",
      "step: 85, loss: 4.451646327972412\n",
      "step: 86, loss: 4.3172101974487305\n",
      "step: 87, loss: 3.7200253009796143\n",
      "step: 88, loss: 3.5441501140594482\n",
      "step: 89, loss: 3.1838440895080566\n",
      "step: 90, loss: 2.9038591384887695\n",
      "step: 91, loss: 2.855569362640381\n",
      "step: 92, loss: 4.581860065460205\n",
      "step: 93, loss: 4.362789154052734\n",
      "step: 94, loss: 4.2939324378967285\n",
      "step: 95, loss: 4.009975910186768\n",
      "step: 96, loss: 3.8524231910705566\n",
      "step: 97, loss: 3.610715389251709\n",
      "step: 98, loss: 3.467951774597168\n",
      "step: 99, loss: 3.2447023391723633\n",
      "step: 100, loss: 3.0144641399383545\n",
      "step: 101, loss: 2.8042643070220947\n",
      "step: 102, loss: 2.602804660797119\n",
      "step: 103, loss: 3.4255785942077637\n",
      "step: 104, loss: 9.667693138122559\n",
      "step: 105, loss: 9.371589660644531\n",
      "step: 106, loss: 9.159610748291016\n",
      "step: 107, loss: 8.986766815185547\n",
      "step: 108, loss: 8.761035919189453\n",
      "step: 109, loss: 8.62346363067627\n",
      "step: 110, loss: 8.202046394348145\n",
      "step: 111, loss: 8.029415130615234\n",
      "step: 112, loss: 7.658167839050293\n",
      "step: 113, loss: 7.415939807891846\n",
      "step: 114, loss: 7.117061614990234\n",
      "step: 115, loss: 7.296212196350098\n",
      "step: 116, loss: 9.099483489990234\n",
      "step: 117, loss: 8.868365287780762\n",
      "step: 118, loss: 8.647225379943848\n",
      "step: 119, loss: 8.425662994384766\n",
      "step: 120, loss: 8.142382621765137\n",
      "step: 121, loss: 7.921273231506348\n",
      "step: 122, loss: 7.577844142913818\n",
      "step: 123, loss: 7.280574321746826\n",
      "step: 124, loss: 6.969094753265381\n",
      "step: 125, loss: 6.360687255859375\n",
      "step: 126, loss: 5.934698104858398\n",
      "step: 127, loss: 5.75192928314209\n",
      "step: 128, loss: 7.449451446533203\n",
      "step: 129, loss: 7.2609405517578125\n",
      "step: 130, loss: 7.060606479644775\n",
      "step: 131, loss: 6.803619861602783\n",
      "step: 132, loss: 6.532403469085693\n",
      "step: 133, loss: 6.227334499359131\n",
      "step: 134, loss: 5.907822132110596\n",
      "step: 135, loss: 5.554500579833984\n",
      "step: 136, loss: 5.135530948638916\n",
      "step: 137, loss: 4.71271276473999\n",
      "step: 138, loss: 4.394721508026123\n",
      "step: 139, loss: 5.84991455078125\n",
      "step: 140, loss: 10.068353652954102\n",
      "step: 141, loss: 9.950401306152344\n",
      "step: 142, loss: 9.762154579162598\n",
      "step: 143, loss: 9.299692153930664\n",
      "step: 144, loss: 9.096571922302246\n",
      "step: 145, loss: 8.704146385192871\n",
      "step: 146, loss: 8.385232925415039\n",
      "step: 147, loss: 8.078739166259766\n",
      "step: 148, loss: 7.7774248123168945\n",
      "step: 149, loss: 7.450246810913086\n",
      "step: 150, loss: 7.294276714324951\n",
      "step: 151, loss: 9.08122444152832\n",
      "step: 152, loss: 10.480145454406738\n",
      "step: 153, loss: 9.826123237609863\n",
      "step: 154, loss: 9.500129699707031\n",
      "step: 155, loss: 9.025385856628418\n",
      "step: 156, loss: 8.767553329467773\n",
      "step: 157, loss: 8.581266403198242\n",
      "step: 158, loss: 8.243195533752441\n",
      "step: 159, loss: 7.914384841918945\n",
      "step: 160, loss: 7.59139347076416\n",
      "step: 161, loss: 7.296038627624512\n",
      "step: 162, loss: 6.973011016845703\n",
      "step: 163, loss: 8.467680931091309\n",
      "step: 164, loss: 9.585335731506348\n",
      "step: 165, loss: 9.351245880126953\n",
      "step: 166, loss: 9.118038177490234\n",
      "step: 167, loss: 8.911609649658203\n",
      "step: 168, loss: 8.597238540649414\n",
      "step: 169, loss: 8.280806541442871\n",
      "step: 170, loss: 7.961629867553711\n",
      "step: 171, loss: 7.559450149536133\n",
      "step: 172, loss: 7.258590221405029\n",
      "step: 173, loss: 6.8595051765441895\n",
      "step: 174, loss: 6.126398086547852\n",
      "step: 175, loss: 5.511171817779541\n",
      "step: 176, loss: 4.8929972648620605\n",
      "step: 177, loss: 4.517349720001221\n",
      "step: 178, loss: 4.033371448516846\n",
      "step: 179, loss: 3.432831048965454\n",
      "step: 180, loss: 2.8849759101867676\n",
      "step: 181, loss: 2.271756887435913\n",
      "step: 182, loss: 1.6881585121154785\n",
      "step: 183, loss: 1.155137538909912\n",
      "step: 184, loss: 0.7160578370094299\n",
      "step: 185, loss: 0.4286899268627167\n",
      "step: 186, loss: 0.2675268352031708\n",
      "step: 187, loss: 6.751935958862305\n",
      "step: 188, loss: 8.88895320892334\n",
      "step: 189, loss: 8.701713562011719\n",
      "step: 190, loss: 8.54298210144043\n",
      "step: 191, loss: 7.8315043449401855\n",
      "step: 192, loss: 7.249422073364258\n",
      "step: 193, loss: 6.588357448577881\n",
      "step: 194, loss: 5.8401594161987305\n",
      "step: 195, loss: 5.011842250823975\n",
      "step: 196, loss: 4.175630569458008\n",
      "step: 197, loss: 3.2965967655181885\n",
      "step: 198, loss: 4.722146511077881\n",
      "step: 199, loss: 4.688808441162109\n",
      "step: 200, loss: 4.201821804046631\n",
      "step: 201, loss: 3.7217013835906982\n",
      "step: 202, loss: 3.288378953933716\n",
      "step: 203, loss: 2.918851852416992\n",
      "step: 204, loss: 2.4354732036590576\n",
      "step: 205, loss: 1.9782891273498535\n",
      "step: 206, loss: 1.4801397323608398\n",
      "step: 207, loss: 1.0474056005477905\n",
      "step: 208, loss: 0.6867325305938721\n",
      "step: 209, loss: 7.435183048248291\n",
      "step: 210, loss: 12.858869552612305\n",
      "step: 211, loss: 12.701613426208496\n",
      "step: 212, loss: 12.478494644165039\n",
      "step: 213, loss: 12.038585662841797\n",
      "step: 214, loss: 11.336546897888184\n",
      "step: 215, loss: 10.09965705871582\n",
      "step: 216, loss: 9.347718238830566\n",
      "step: 217, loss: 8.0381498336792\n",
      "step: 218, loss: 9.52623176574707\n",
      "step: 219, loss: 15.281423568725586\n",
      "step: 220, loss: 13.099096298217773\n",
      "step: 221, loss: 12.19359016418457\n",
      "step: 222, loss: 10.697700500488281\n",
      "step: 223, loss: 10.141587257385254\n",
      "step: 224, loss: 9.650694847106934\n",
      "step: 225, loss: 9.306122779846191\n",
      "step: 226, loss: 3.315908193588257\n",
      "step: 227, loss: 3.0706968307495117\n",
      "step: 228, loss: 2.743325710296631\n",
      "step: 229, loss: 2.892594575881958\n",
      "step: 230, loss: 2.883502721786499\n",
      "step: 231, loss: 2.963850498199463\n",
      "step: 232, loss: 2.7212133407592773\n",
      "step: 233, loss: 2.8224730491638184\n",
      "step: 234, loss: 3.5583231449127197\n",
      "step: 235, loss: 2.606160879135132\n",
      "step: 236, loss: 2.968303918838501\n",
      "step: 237, loss: 3.07039737701416\n",
      "step: 238, loss: 3.0679666996002197\n",
      "step: 239, loss: 2.859114646911621\n",
      "step: 240, loss: 2.621695041656494\n",
      "step: 241, loss: 2.4811713695526123\n",
      "step: 242, loss: 2.6207337379455566\n",
      "step: 243, loss: 2.6949362754821777\n",
      "step: 244, loss: 2.5535476207733154\n",
      "step: 245, loss: 2.4912290573120117\n",
      "step: 246, loss: 2.2975003719329834\n",
      "step: 247, loss: 2.432983875274658\n",
      "step: 248, loss: 2.1586360931396484\n",
      "step: 249, loss: 2.7904553413391113\n",
      "step: 250, loss: 2.431509017944336\n",
      "step: 251, loss: 2.134427309036255\n",
      "step: 252, loss: 2.2268128395080566\n",
      "step: 253, loss: 2.0387277603149414\n",
      "step: 254, loss: 2.3431763648986816\n",
      "step: 255, loss: 2.043337345123291\n",
      "step: 256, loss: 1.955424189567566\n",
      "step: 257, loss: 2.1834981441497803\n",
      "step: 258, loss: 2.4863193035125732\n",
      "step: 259, loss: 2.0285749435424805\n",
      "step: 260, loss: 1.7579514980316162\n",
      "step: 261, loss: 1.983317255973816\n",
      "step: 262, loss: 1.9177862405776978\n",
      "step: 263, loss: 1.8620930910110474\n",
      "step: 264, loss: 1.8552783727645874\n",
      "step: 265, loss: 1.8207110166549683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 266, loss: 1.493019700050354\n",
      "step: 267, loss: 2.1526834964752197\n",
      "step: 268, loss: 1.8244781494140625\n",
      "step: 269, loss: 1.9756704568862915\n",
      "step: 270, loss: 2.0627267360687256\n",
      "step: 271, loss: 1.5928928852081299\n",
      "step: 272, loss: 1.4320051670074463\n",
      "step: 273, loss: 1.5567641258239746\n",
      "step: 274, loss: 1.5367789268493652\n",
      "step: 275, loss: 1.8857799768447876\n",
      "step: 276, loss: 1.8771836757659912\n",
      "step: 277, loss: 1.3745571374893188\n",
      "step: 278, loss: 1.0902302265167236\n",
      "step: 279, loss: 1.4528682231903076\n",
      "step: 280, loss: 1.5016686916351318\n",
      "step: 281, loss: 1.350235939025879\n",
      "step: 282, loss: 1.2450170516967773\n",
      "step: 283, loss: 1.3291057348251343\n",
      "step: 284, loss: 1.762158989906311\n",
      "step: 285, loss: 0.8987589478492737\n",
      "step: 286, loss: 1.5216830968856812\n",
      "step: 287, loss: 1.345597743988037\n",
      "step: 288, loss: 1.4678940773010254\n",
      "step: 289, loss: 1.2989262342453003\n",
      "step: 290, loss: 1.3466438055038452\n",
      "step: 291, loss: 1.586521863937378\n",
      "step: 292, loss: 1.4677730798721313\n",
      "step: 293, loss: 1.1381959915161133\n",
      "step: 294, loss: 1.3946250677108765\n",
      "step: 295, loss: 1.29457426071167\n",
      "step: 296, loss: 1.2630647420883179\n",
      "step: 297, loss: 1.0506792068481445\n",
      "step: 298, loss: 1.1318475008010864\n",
      "step: 299, loss: 1.579148769378662\n",
      "step: 300, loss: 1.0129332542419434\n",
      "step: 301, loss: 0.6551403999328613\n",
      "step: 302, loss: 1.0682371854782104\n",
      "step: 303, loss: 1.331998348236084\n",
      "step: 304, loss: 1.2182329893112183\n",
      "step: 305, loss: 1.1652799844741821\n",
      "step: 306, loss: 0.9858150482177734\n",
      "step: 307, loss: 0.8595580458641052\n",
      "step: 308, loss: 1.2636604309082031\n",
      "step: 309, loss: 0.8103774785995483\n",
      "step: 310, loss: 0.8890922665596008\n",
      "step: 311, loss: 1.3208409547805786\n",
      "step: 312, loss: 0.8275437355041504\n",
      "step: 313, loss: 0.6474912762641907\n",
      "step: 314, loss: 1.0810143947601318\n",
      "step: 315, loss: 1.198129415512085\n",
      "step: 316, loss: 1.2550106048583984\n",
      "step: 317, loss: 0.9203569889068604\n",
      "step: 318, loss: 0.8994433879852295\n",
      "step: 319, loss: 1.0771571397781372\n",
      "step: 320, loss: 0.8922005295753479\n",
      "step: 321, loss: 0.7772225737571716\n",
      "step: 322, loss: 0.9532882571220398\n",
      "step: 323, loss: 0.7973483204841614\n",
      "step: 324, loss: 0.707366943359375\n",
      "step: 325, loss: 0.7939186096191406\n",
      "step: 326, loss: 0.6440377831459045\n",
      "step: 327, loss: 0.600367546081543\n",
      "step: 328, loss: 0.9488903880119324\n",
      "step: 329, loss: 0.5716936588287354\n",
      "step: 330, loss: 1.149436354637146\n",
      "step: 331, loss: 0.6961119771003723\n",
      "step: 332, loss: 0.7467654943466187\n",
      "step: 333, loss: 0.7087892293930054\n",
      "step: 334, loss: 0.8680322766304016\n",
      "step: 335, loss: 0.6391909122467041\n",
      "step: 336, loss: 0.8128158450126648\n",
      "step: 337, loss: 0.9938368201255798\n",
      "step: 338, loss: 0.5268053412437439\n",
      "step: 339, loss: 0.9561975002288818\n",
      "step: 340, loss: 0.7413861155509949\n",
      "step: 341, loss: 0.7361910939216614\n",
      "step: 342, loss: 0.7098159790039062\n",
      "step: 343, loss: 0.6871441602706909\n",
      "step: 344, loss: 0.6179993748664856\n",
      "step: 345, loss: 0.5932542681694031\n",
      "step: 346, loss: 0.5269613265991211\n",
      "step: 347, loss: 0.8712610602378845\n",
      "step: 348, loss: 1.0198880434036255\n",
      "step: 349, loss: 0.5640153288841248\n",
      "step: 350, loss: 0.9823555946350098\n",
      "step: 351, loss: 0.6086781024932861\n",
      "step: 352, loss: 0.49939632415771484\n",
      "step: 353, loss: 0.7245793342590332\n",
      "step: 354, loss: 0.9241670966148376\n",
      "step: 355, loss: 0.8898080587387085\n",
      "step: 356, loss: 0.4551345407962799\n",
      "step: 357, loss: 0.7521181702613831\n",
      "step: 358, loss: 0.50070720911026\n",
      "step: 359, loss: 0.6766073107719421\n",
      "step: 360, loss: 0.5844157934188843\n",
      "step: 361, loss: 0.5111247897148132\n",
      "step: 362, loss: 0.666974663734436\n",
      "step: 363, loss: 0.830845057964325\n",
      "step: 364, loss: 0.479172945022583\n",
      "step: 365, loss: 0.9107796549797058\n",
      "step: 366, loss: 0.4511912167072296\n",
      "step: 367, loss: 0.4089161157608032\n",
      "step: 368, loss: 0.6710326671600342\n",
      "step: 369, loss: 0.49835193157196045\n",
      "step: 370, loss: 0.5303254127502441\n",
      "step: 371, loss: 0.4925173819065094\n",
      "step: 372, loss: 0.2246522456407547\n",
      "step: 373, loss: 0.3654770255088806\n",
      "step: 374, loss: 0.6154683828353882\n",
      "step: 375, loss: 0.5294854640960693\n",
      "step: 376, loss: 0.776914656162262\n",
      "step: 377, loss: 0.6260638236999512\n",
      "step: 378, loss: 0.48937928676605225\n",
      "step: 379, loss: 0.6171163320541382\n",
      "step: 380, loss: 0.1545284241437912\n",
      "step: 381, loss: 0.9419345855712891\n",
      "step: 382, loss: 0.5245050191879272\n",
      "step: 383, loss: 0.8285313248634338\n",
      "step: 384, loss: 0.521979033946991\n",
      "step: 385, loss: 0.8756354451179504\n",
      "step: 386, loss: 0.45897892117500305\n",
      "step: 387, loss: 0.6034492254257202\n",
      "step: 388, loss: 0.42664578557014465\n",
      "step: 389, loss: 0.4826430082321167\n",
      "step: 390, loss: 0.5249102115631104\n",
      "step: 391, loss: 0.5574948787689209\n",
      "step: 392, loss: 0.3990880250930786\n",
      "step: 393, loss: 0.28280261158943176\n",
      "step: 394, loss: 0.7353077530860901\n",
      "step: 395, loss: 0.6219941973686218\n",
      "step: 396, loss: 0.7635977268218994\n",
      "step: 397, loss: 0.5780683755874634\n",
      "step: 398, loss: 0.4226815104484558\n",
      "step: 399, loss: 0.23936277627944946\n",
      "step: 400, loss: 0.44172677397727966\n",
      "step: 401, loss: 0.7395915985107422\n",
      "step: 402, loss: 0.6404420733451843\n",
      "step: 403, loss: 0.28167369961738586\n",
      "step: 404, loss: 0.6529671549797058\n",
      "step: 405, loss: 0.33794277906417847\n",
      "step: 406, loss: 0.8721402883529663\n",
      "step: 407, loss: 0.5170710682868958\n",
      "step: 408, loss: 0.6962100863456726\n",
      "step: 409, loss: 0.9108679890632629\n",
      "step: 410, loss: 0.34695935249328613\n",
      "step: 411, loss: 0.12003741413354874\n",
      "step: 412, loss: 0.31079787015914917\n",
      "step: 413, loss: 0.5388798117637634\n",
      "step: 414, loss: 0.34921279549598694\n",
      "step: 415, loss: 0.7595623135566711\n",
      "step: 416, loss: 0.4859916567802429\n",
      "step: 417, loss: 0.5397586822509766\n",
      "step: 418, loss: 0.33347269892692566\n",
      "step: 419, loss: 0.6233691573143005\n",
      "step: 420, loss: 0.44758090376853943\n",
      "step: 421, loss: 0.20627525448799133\n",
      "step: 422, loss: 0.1773241013288498\n",
      "step: 423, loss: 0.3887431025505066\n",
      "step: 424, loss: 0.47436729073524475\n",
      "step: 425, loss: 0.48710742592811584\n",
      "step: 426, loss: 0.3418140709400177\n",
      "step: 427, loss: 0.6587217450141907\n",
      "step: 428, loss: 0.4784806966781616\n",
      "step: 429, loss: 0.32349368929862976\n",
      "step: 430, loss: 0.3222333788871765\n",
      "step: 431, loss: 0.4172724485397339\n",
      "step: 432, loss: 0.38835933804512024\n",
      "step: 433, loss: 0.6837491393089294\n",
      "step: 434, loss: 0.08491985499858856\n",
      "step: 435, loss: 0.29738667607307434\n",
      "step: 436, loss: 0.5023528933525085\n",
      "step: 437, loss: 0.22891657054424286\n",
      "step: 438, loss: 0.6341848969459534\n",
      "step: 439, loss: 0.846782386302948\n",
      "step: 440, loss: 0.664553701877594\n",
      "step: 441, loss: 0.4451583921909332\n",
      "step: 442, loss: 0.37992382049560547\n",
      "step: 443, loss: 0.4837256669998169\n",
      "step: 444, loss: 0.4852519631385803\n",
      "step: 445, loss: 0.3353642225265503\n",
      "step: 446, loss: 0.27823561429977417\n",
      "step: 447, loss: 0.38363057374954224\n",
      "step: 448, loss: 0.3991289436817169\n",
      "step: 449, loss: 0.7190742492675781\n",
      "step: 450, loss: 0.22519145905971527\n",
      "step: 451, loss: 0.28866279125213623\n",
      "step: 452, loss: 0.2745194435119629\n",
      "step: 453, loss: 0.22504922747612\n",
      "step: 454, loss: 0.3915582001209259\n",
      "step: 455, loss: 0.22426094114780426\n",
      "step: 456, loss: 0.1796833574771881\n",
      "step: 457, loss: 0.09738057851791382\n",
      "step: 458, loss: 0.10955049842596054\n",
      "step: 459, loss: 0.25642529129981995\n",
      "step: 460, loss: 0.11120374500751495\n",
      "step: 461, loss: 0.18892964720726013\n",
      "step: 462, loss: 0.21972540020942688\n",
      "step: 463, loss: 0.05400875583291054\n",
      "step: 464, loss: 0.39858654141426086\n",
      "step: 465, loss: 0.3542056977748871\n",
      "step: 466, loss: 0.25317713618278503\n",
      "step: 467, loss: 0.23517589271068573\n",
      "step: 468, loss: 0.14153523743152618\n",
      "step: 469, loss: 0.27721282839775085\n",
      "step: 470, loss: 0.25780510902404785\n",
      "step: 471, loss: 0.307194322347641\n",
      "step: 472, loss: 0.10134278982877731\n",
      "step: 473, loss: 0.17306581139564514\n",
      "step: 474, loss: 0.041136790066957474\n",
      "step: 475, loss: 0.054349932819604874\n",
      "step: 476, loss: 0.17930103838443756\n",
      "step: 477, loss: 0.21549393236637115\n",
      "step: 478, loss: 0.11857257038354874\n",
      "step: 479, loss: 0.06353066861629486\n",
      "step: 480, loss: 0.13043858110904694\n",
      "step: 481, loss: 0.10518917441368103\n",
      "step: 482, loss: 0.19903838634490967\n",
      "step: 483, loss: 0.3377295732498169\n",
      "step: 484, loss: 0.04211137816309929\n",
      "step: 485, loss: 0.11465627700090408\n",
      "step: 486, loss: 0.23005211353302002\n",
      "step: 487, loss: 0.2066565752029419\n",
      "step: 488, loss: 0.21337991952896118\n",
      "step: 489, loss: 0.07231204211711884\n",
      "step: 490, loss: 0.1348157823085785\n",
      "step: 491, loss: 0.17246614396572113\n",
      "step: 492, loss: 0.21483659744262695\n",
      "step: 493, loss: 0.1805093139410019\n",
      "step: 494, loss: 0.2833641469478607\n",
      "step: 495, loss: 0.34398627281188965\n",
      "step: 496, loss: 0.23144018650054932\n",
      "step: 497, loss: 0.048344679176807404\n",
      "step: 498, loss: 0.2453063428401947\n",
      "step: 499, loss: 0.13528086245059967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Accuracy on test data: 0.7635422198619225\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"NUM_CLASSES = 20\\nwith open(os.getcwd() + '/20news-bydate/20news-full-words-idfs.txt') as f:\\n    vocab_size = len(f.read().splitlines())\\nmlp = MLP(vocab_size = vocab_size, hidden_size = 50)\\npredicted_labels, loss = mlp.build_graph()\\ntrain_op = mlp.trainer(loss = loss, learning_rate = 0.1)\\nwith tf.Session() as sess:\\n    train_data_reader, test_data_reader = load_dataset(vocab_size)\\n    max_step = 500\\n    sess.run(tf.global_variables_initializer())\\n    for step in range(max_step):\\n        train_data, train_labels = train_data_reader.next_batch()\\n        labels_eval, loss_eval, _ = sess.run(\\n            [predicted_labels, loss, train_op],\\n            feed_dict={\\n                mlp._X: train_data,\\n                mlp._real_Y: train_labels\\n            }\\n        )\\n        print('step: {}, loss: {}'.format(step, loss_eval))\\n        if (train_data_reader._batch_id == 0):\\n            trainable_variables = tf.trainable_variables()\\n            for variable in trainable_variables:\\n                save_parameters(\\n                name = variable.name,\\n                value = variable.eval(),\\n                epoch = train_data_reader._num_epoch\\n            )\\nwith tf.Session() as sess:\\n    epoch = train_data_reader._num_epoch\\n    trainable_variables = tf.trainable_variables()\\n    for variable in trainable_variables:\\n        saved_value = restore_parameters(variable.name,epoch)\\n        assign_op = variable.assign(saved_value)\\n        sess.run(assign_op)\\n    num_true_preds = 0\\n    while True:  \\n        test_data, test_labels = test_data_reader.next_batch()\\n        test_labels_eval = sess.run(\\n            predicted_labels,\\n            feed_dict = {\\n                mlp._X: test_data,\\n                mlp._real_Y: test_labels\\n            }\\n        )\\n        matches = np.equal(test_labels_eval, test_labels)\\n        num_true_preds += np.sum(matches.astype(float))\\n        if test_data_reader._batch_id == 0:\\n            break\\n    print('Epoch:', epoch)\\n    print('Accuracy on test data:', num_true_preds/len(test_data_reader._data))\";\n",
       "                var nbb_formatted_code = \"NUM_CLASSES = 20\\nwith open(os.getcwd() + \\\"/20news-bydate/20news-full-words-idfs.txt\\\") as f:\\n    vocab_size = len(f.read().splitlines())\\nmlp = MLP(vocab_size=vocab_size, hidden_size=50)\\npredicted_labels, loss = mlp.build_graph()\\ntrain_op = mlp.trainer(loss=loss, learning_rate=0.1)\\nwith tf.Session() as sess:\\n    train_data_reader, test_data_reader = load_dataset(vocab_size)\\n    max_step = 500\\n    sess.run(tf.global_variables_initializer())\\n    for step in range(max_step):\\n        train_data, train_labels = train_data_reader.next_batch()\\n        labels_eval, loss_eval, _ = sess.run(\\n            [predicted_labels, loss, train_op],\\n            feed_dict={mlp._X: train_data, mlp._real_Y: train_labels},\\n        )\\n        print(\\\"step: {}, loss: {}\\\".format(step, loss_eval))\\n        if train_data_reader._batch_id == 0:\\n            trainable_variables = tf.trainable_variables()\\n            for variable in trainable_variables:\\n                save_parameters(\\n                    name=variable.name,\\n                    value=variable.eval(),\\n                    epoch=train_data_reader._num_epoch,\\n                )\\nwith tf.Session() as sess:\\n    epoch = train_data_reader._num_epoch\\n    trainable_variables = tf.trainable_variables()\\n    for variable in trainable_variables:\\n        saved_value = restore_parameters(variable.name, epoch)\\n        assign_op = variable.assign(saved_value)\\n        sess.run(assign_op)\\n    num_true_preds = 0\\n    while True:\\n        test_data, test_labels = test_data_reader.next_batch()\\n        test_labels_eval = sess.run(\\n            predicted_labels, feed_dict={mlp._X: test_data, mlp._real_Y: test_labels}\\n        )\\n        matches = np.equal(test_labels_eval, test_labels)\\n        num_true_preds += np.sum(matches.astype(float))\\n        if test_data_reader._batch_id == 0:\\n            break\\n    print(\\\"Epoch:\\\", epoch)\\n    print(\\\"Accuracy on test data:\\\", num_true_preds / len(test_data_reader._data))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_CLASSES = 20\n",
    "with open(os.getcwd() + '/20news-bydate/20news-full-words-idfs.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "mlp = MLP(vocab_size = vocab_size, hidden_size = 50)\n",
    "predicted_labels, loss = mlp.build_graph()\n",
    "train_op = mlp.trainer(loss = loss, learning_rate = 0.1)\n",
    "with tf.Session() as sess:\n",
    "    train_data_reader, test_data_reader = load_dataset(vocab_size)\n",
    "    max_step = 500\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(max_step):\n",
    "        train_data, train_labels = train_data_reader.next_batch()\n",
    "        labels_eval, loss_eval, _ = sess.run(\n",
    "            [predicted_labels, loss, train_op],\n",
    "            feed_dict={\n",
    "                mlp._X: train_data,\n",
    "                mlp._real_Y: train_labels\n",
    "            }\n",
    "        )\n",
    "        print('step: {}, loss: {}'.format(step, loss_eval))\n",
    "        if (train_data_reader._batch_id == 0):\n",
    "            trainable_variables = tf.trainable_variables()\n",
    "            for variable in trainable_variables:\n",
    "                save_parameters(\n",
    "                name = variable.name,\n",
    "                value = variable.eval(),\n",
    "                epoch = train_data_reader._num_epoch\n",
    "            )\n",
    "with tf.Session() as sess:\n",
    "    epoch = train_data_reader._num_epoch\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    for variable in trainable_variables:\n",
    "        saved_value = restore_parameters(variable.name,epoch)\n",
    "        assign_op = variable.assign(saved_value)\n",
    "        sess.run(assign_op)\n",
    "    num_true_preds = 0\n",
    "    while True:  \n",
    "        test_data, test_labels = test_data_reader.next_batch()\n",
    "        test_labels_eval = sess.run(\n",
    "            predicted_labels,\n",
    "            feed_dict = {\n",
    "                mlp._X: test_data,\n",
    "                mlp._real_Y: test_labels\n",
    "            }\n",
    "        )\n",
    "        matches = np.equal(test_labels_eval, test_labels)\n",
    "        num_true_preds += np.sum(matches.astype(float))\n",
    "        if test_data_reader._batch_id == 0:\n",
    "            break\n",
    "    print('Epoch:', epoch)\n",
    "    print('Accuracy on test data:', num_true_preds/len(test_data_reader._data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
